{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365be60a",
   "metadata": {},
   "source": [
    "# Unicycle Go To Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58622d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "from dimp.robots import (\n",
    "    MPCData, UniInput, UniRobot, UniState\n",
    ")\n",
    "\n",
    "from dimp.utils import init_matplotlib\n",
    "\n",
    "\n",
    "init_matplotlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56085ce6",
   "metadata": {},
   "source": [
    "### Create The Data for the MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 3      # Number of states (x, y, theta)\n",
    "ni = 2      # Number of inputs (v, omega)\n",
    "\n",
    "nc = 50     # Number of control intervals\n",
    "\n",
    "s0 = cp.Parameter(ns, name=\"s0\")\n",
    "\n",
    "mpc_data = MPCData(\n",
    "    nc=nc,\n",
    "    states_list=[UniState(s0)] + [UniState(cp.Variable(ns, name=f\"s{k+1}\")) for k in range(nc)],\n",
    "    statesbar_list=[UniState(s0)] + [UniState(cp.Parameter(ns)) for _ in range(nc)],\n",
    "    inputs_list=[UniInput(cp.Variable(ni, name=f\"i{k}\")) for k in range(nc)],\n",
    "    inputsbar_list=[UniInput(cp.Parameter(ni)) for _ in range(nc)],\n",
    ")\n",
    "\n",
    "dt = 0.1\n",
    "robot = UniRobot(dt=dt, mpc_data=mpc_data)\n",
    "\n",
    "# Parameters\n",
    "p_goal = np.array([10.0, 0.0])\n",
    "v_max = 10.0\n",
    "omega_max = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b770338",
   "metadata": {},
   "source": [
    "### Create The MPC Problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\substack{s_k, u_k \\\\ \\quad k=1, \\, \\dots, \\, n_c}} \\quad && \\frac{1}{2} \\sum_{k=1}^{n_c} (p_k - p_\\text{goal})^2 + w_1 u_k^2 + w_2 \\left( (s_k - \\bar{s}_k)^2 + (u_k - \\bar{u}_k)^2 \\right) \\\\\n",
    "&\\text{s.t.} && s_{k+1} = A_k(s_{bar, k}, u_{bar, k}) s_k + B_k(s_{bar, k}, u_{bar, k}) u_k, \\\\\n",
    "& && \\left| u_k \\right| \\leq u_\\text{max}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = cp.Parameter(1, name=\"w1\", nonneg=True)\n",
    "w2 = cp.Parameter(1, name=\"w2\", nonneg=True)\n",
    "\n",
    "def create_qp(nc: int):\n",
    "    objective = cp.Minimize(\n",
    "          0.5 * cp.sum_squares(cp.hstack([mpc_data.statei[i+1][0:2] - p_goal for i in range(nc)]))\n",
    "        + 0.5 * w1 * cp.sum_squares(cp.hstack([mpc_data.inputi[i] for i in range(nc)]))\n",
    "        + 0.5 * w2 * cp.sum_squares(cp.hstack([mpc_data.statei[i+1] - mpc_data.statebari[i+1].value for i in range(nc)]))\n",
    "        + 0.5 * w2 * cp.sum_squares(cp.hstack([mpc_data.inputi[i] - mpc_data.inputbari[i].value for i in range(nc)]))\n",
    "    )\n",
    "\n",
    "    dynamics_constraints = robot.dt_dynamics_constraint()\n",
    "\n",
    "    input_constraints = [cp.abs(mpc_data.inputi[i][0]) <= v_max for i in range(nc)] + \\\n",
    "        [cp.abs(mpc_data.inputi[i][1]) <= omega_max for i in range(nc)]\n",
    "\n",
    "    constraints = dynamics_constraints + input_constraints\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return problem\n",
    "\n",
    "qp_problem = create_qp(nc=nc)\n",
    "assert qp_problem.is_dpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_to(\n",
    "    create_problem: callable,\n",
    "    mpc_data: MPCData,\n",
    "    n_iters: int=1,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate the robot's motion over the OCP horizon (trajectory optimization).\n",
    "    The OCP is solved once and the entire state-input trajectory is obtained in one go.\n",
    "\n",
    "    Args:\n",
    "        problem (cp.Problem): The optimization problem to solve.\n",
    "        mpc_data (MPCData): The MPC data containing state and input variables.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arrays of states and inputs over the simulation steps of size [steps, ns] and [steps, ni], respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    states = np.zeros((mpc_data.nc+1, ns))\n",
    "    inputs = np.zeros((mpc_data.nc, ni))\n",
    "    \n",
    "    mpc_data.statei[0].value = np.array([0.0, 0.0, 1.0])\n",
    "    for i in range(mpc_data.nc):\n",
    "        mpc_data.statei[i+1].value = np.array([0.0, 0.0, 1.0])\n",
    "        mpc_data.inputi[i].value = np.array([0.0, 0.0])\n",
    "    mpc_data.update_bar()\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        problem = create_problem(nc=nc)\n",
    "        problem.solve()\n",
    "        mpc_data.update_bar()\n",
    "    \n",
    "    states[0, :] = mpc_data.statei[0].value\n",
    "    for i in range(mpc_data.nc):\n",
    "        states[i+1, :] = mpc_data.statei[i+1].value\n",
    "        inputs[i, :] = mpc_data.inputi[i].value\n",
    "\n",
    "    return states, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb12a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_mpc(\n",
    "    problem: cp.Problem,\n",
    "    mpc_data: MPCData,\n",
    "    n_steps: int = 200,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate the robot's motion over a fixed number of steps (MPC).\n",
    "    The OCP is solved multiple times and only the first control input is applied at each step.\n",
    "\n",
    "    Args:\n",
    "        problem (cp.Problem): The optimization problem to solve.\n",
    "        mpc_data (MPCData): The MPC data containing state and input variables.\n",
    "        n_steps (int): The number of steps to simulate.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arrays of states and inputs over the simulation steps of size [steps, ns] and [steps, ni], respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    states = np.zeros((n_steps, ns))\n",
    "    inputs = np.zeros((n_steps, ni))\n",
    "\n",
    "    mpc_data.statei[0].value = np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        problem.solve()\n",
    "\n",
    "        mpc_data.statei[0].value = mpc_data.statei[1].value\n",
    "\n",
    "        mpc_data.update_bar()\n",
    "\n",
    "        states[i, :] = mpc_data.statei[1].value\n",
    "        inputs[i, :] = mpc_data.inputi[1].value\n",
    "\n",
    "    return states, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6289a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _triangle_xy(x, y, theta, L=0.6, W=0.35):\n",
    "    pts = np.array([\n",
    "        [ + L/2.0,   0.0   ],  # tip\n",
    "        [ - L/2.0, + W/2.0 ],  # rear-left\n",
    "        [ - L/2.0, - W/2.0 ],  # rear-right\n",
    "        [ + L/2.0,   0.0   ],  # close\n",
    "    ])\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c, -s],[s, c]])\n",
    "    rot = pts @ R.T\n",
    "    rot[:, 0] += x\n",
    "    rot[:, 1] += y\n",
    "    return rot[:, 0], rot[:, 1]\n",
    "\n",
    "def plot_trajectory(states: np.ndarray, body_len=0.6, body_w=0.35) -> None:\n",
    "    # x, y, heading(rad) assumed at [:,0], [:,1], [:,3]\n",
    "    steps = states.shape[0]\n",
    "    thetas = np.unwrap(states[:, 2].astype(float))  # keep radians; unwrap for smooth rotation\n",
    "\n",
    "    xm, xM = states[:, 0].min() - 1, states[:, 0].max() + 1\n",
    "    ym, yM = states[:, 1].min() - 1, states[:, 1].max() + 1\n",
    "\n",
    "    # colors\n",
    "    line_blue = \"rgba(0, 0, 255, 0.85)\"\n",
    "    fill_blue = \"rgba(0, 0, 255, 0.35)\"\n",
    "\n",
    "    # initial triangle\n",
    "    tri_x0, tri_y0 = _triangle_xy(states[0, 0], states[0, 1], thetas[0], body_len, body_w)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            # 0) trajectory (blue)\n",
    "            go.Scatter(\n",
    "                x=states[:, 0], y=states[:, 1],\n",
    "                mode=\"lines\", name=\"Trajectory\",\n",
    "                line=dict(width=2, color=\"rgba(0, 0, 255, 0.5)\", dash=\"dot\")\n",
    "            ),\n",
    "            # 1) vehicle triangle (animated trace) — blue\n",
    "            go.Scatter(\n",
    "                x=tri_x0, y=tri_y0,\n",
    "                mode=\"lines\", fill=\"toself\",\n",
    "                line=dict(width=1, color=line_blue),\n",
    "                fillcolor=fill_blue,\n",
    "                opacity=1.0,\n",
    "                hoverinfo=\"skip\",\n",
    "                showlegend=False,            # hide this from legend…\n",
    "                name=\"Vehicle\"\n",
    "            ),\n",
    "            # 2) legend-only proxy with a triangle icon\n",
    "            go.Scatter(\n",
    "                x=[states[0, 0]], y=[states[0, 1]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(symbol=\"triangle-right\", size=12, color=line_blue),\n",
    "                name=\"Vehicle\",\n",
    "                visible=\"legendonly\",        # …and show this only in the legend\n",
    "                hoverinfo=\"skip\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # layout + slow animation\n",
    "    fig.update_layout(\n",
    "        width=600, height=450,\n",
    "        xaxis=dict(range=[xm, xM], autorange=False, zeroline=False, scaleanchor=\"y\", title=\"x [m]\"),\n",
    "        yaxis=dict(range=[ym, yM], autorange=False, zeroline=False, title=\"y [m]\"),\n",
    "        title_text=\"Trajectory\", title_x=0.5,\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[dict(\n",
    "                args=[None, {\n",
    "                    \"frame\": {\"duration\": 100, \"redraw\": False},  # slower\n",
    "                    \"fromcurrent\": True,\n",
    "                    \"transition\": {\"duration\": 0},\n",
    "                    \"mode\": \"immediate\"\n",
    "                }],\n",
    "                label=\"Play\",\n",
    "                method=\"animate\",\n",
    "            )]\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    # frames: update only the triangle trace (index 1)\n",
    "    fig.update(frames=[\n",
    "        go.Frame(\n",
    "            data=[go.Scatter(*(), x=_triangle_xy(states[k, 0], states[k, 1], thetas[k], body_len, body_w)[0],\n",
    "                                  y=_triangle_xy(states[k, 0], states[k, 1], thetas[k], body_len, body_w)[1])],\n",
    "            traces=[1]\n",
    "        ) for k in range(steps)\n",
    "    ])\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(\"uni_robot_mpc.html\", include_plotlyjs=\"cdn\", full_html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_plot_qp():\n",
    "    w1.value = np.array([1.0])\n",
    "    w2.value = np.array([0.1])\n",
    "    \n",
    "    for n_iters in range(5):\n",
    "        states, inputs = simulate_to(create_qp, mpc_data, n_iters=n_iters+1)\n",
    "        \n",
    "        plot_trajectory(states)\n",
    "\n",
    "simulate_and_plot_qp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51a3af",
   "metadata": {},
   "source": [
    "### Create the `CvxpyLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75567e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_loss(states, inputs):\n",
    "    p_goal_th = torch.tensor(p_goal, requires_grad=True)\n",
    "    \n",
    "    st_cost = torch.stack([(s[0:2] - p_goal_th).pow(2).sum() for s in states]).sum()\n",
    "    in_cost = torch.stack([u.pow(2).sum()               for u in inputs]).sum()\n",
    "    return 0.5 * st_cost + 0.5 * 0.1 * in_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e33176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_res(history):\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot([h['loss'] for h in history])\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_title(\"Loss Evolution\")\n",
    "\n",
    "    ax[1].plot([h['w'][0] for h in history])\n",
    "    ax[1].set_title(\"Weights Evolution\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(r\"$w_0$\")\n",
    "    ax[1].tick_params(axis='y')\n",
    "\n",
    "    fig.set_constrained_layout(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvxpylayer = CvxpyLayer(\n",
    "    qp_problem,\n",
    "    parameters=[w1, w2, s0],\n",
    "    variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    ")\n",
    "\n",
    "w1_th = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "w2_th = torch.tensor([1.0])\n",
    "s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "\n",
    "optim = torch.optim.Adam([w1_th], lr=5e-3)\n",
    "\n",
    "solution = cvxpylayer(w1_th, w2_th, s0_th)\n",
    "\n",
    "solution[0].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e52fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "history  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1_th.copy_(torch.tensor([1.0]))\n",
    "    w2_th.copy_(torch.tensor([1.0]))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "    \n",
    "    qp_problem = create_qp(nc=nc)\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        qp_problem,\n",
    "        parameters=[w1, w2, s0],\n",
    "        variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    "    )\n",
    "\n",
    "    sol = cvxpylayer(w1_th, w2_th, s0_th)\n",
    "    states = [sol[i] for i in range(nc)]\n",
    "    inputs = [sol[nc + i] for i in range(nc)]\n",
    "    \n",
    "    mpc_data.statebari[0].value = s0_th.detach().cpu().numpy()\n",
    "    for i in range(nc):\n",
    "        mpc_data.statebari[i+1].value = states[i].detach().cpu().numpy()\n",
    "        mpc_data.inputbari[i].value = inputs[i].detach().cpu().numpy()\n",
    "\n",
    "    loss = task_loss(states, inputs)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    with torch.no_grad():\n",
    "        w1_th.clamp_(min=1e-6)\n",
    "\n",
    "    history.append({\n",
    "        'loss': loss.item(),\n",
    "        'w': copy.deepcopy(w1_th.detach().cpu().numpy()),\n",
    "        'dw': copy.deepcopy(w1_th.grad.detach().cpu().numpy()),\n",
    "    })\n",
    "    print(f\"Epoch {epoch:2d} | loss = {loss.item():.4f} | w = {w1_th.detach().cpu().numpy()}\")\n",
    "    \n",
    "plot_training_res(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a4486",
   "metadata": {},
   "source": [
    "## Dataset Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e53b45",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "Solve a simple SQP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mpc_data\n",
    "def init_mpc_data(mpc_data):\n",
    "    s0_np = np.array([0.0, 0.0, 1.0])\n",
    "    sd_np = np.array([10.0, 0.0, 0.0])\n",
    "    mpc_data.statei[0].value = s0_np\n",
    "    for i in range(nc):\n",
    "        mpc_data.statei[i+1].value = s0_np + (sd_np - s0_np) * (i+1)/nc\n",
    "        mpc_data.inputi[i].value = np.array([10.0, 0.0])\n",
    "    mpc_data.update_bar()\n",
    "init_mpc_data(mpc_data)\n",
    "\n",
    "# Set the optimal gains\n",
    "w1.value = np.array([1.0])\n",
    "w2.value = np.array([0.1])\n",
    "\n",
    "# Initialize dataset vars\n",
    "states_ds = torch.zeros((mpc_data.nc+1, ns))\n",
    "inputs_ds = torch.zeros((mpc_data.nc, ni))\n",
    "\n",
    "# Solve an SQP\n",
    "for i in range(5):\n",
    "    problem = create_qp(nc=nc)\n",
    "    problem.solve()\n",
    "    mpc_data.update_bar()\n",
    "\n",
    "# Extract the states and inputs to create the dataset\n",
    "states_ds[0, :] = torch.tensor(mpc_data.statei[0].value)\n",
    "for i in range(mpc_data.nc):\n",
    "    states_ds[i+1, :] = torch.tensor(mpc_data.statei[i+1].value)\n",
    "    inputs_ds[i, :] = torch.tensor(mpc_data.inputi[i].value)\n",
    "plot_trajectory(np.array(states_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef318ad",
   "metadata": {},
   "source": [
    "## QP Problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\substack{s_k, u_k \\\\ \\quad k=1, \\, \\dots, \\, n_c}} \\quad && \\frac{1}{2} \\sum_{k=1}^{n_c} (s_k - s_\\text{goal})^2_Q + (u_k)^2_R + (s_k - \\bar{s}_k)^2_{Q_bar} + (u_k - \\bar{u}_k)^2_{R_bar} \\\\\n",
    "&\\text{s.t.} && s_{k+1} = A_k(\\bar{s}_k, \\bar{u}_k) s_k + B_k(\\bar{s}_k, \\bar{u}_k) u_k, \\\\\n",
    "& && \\left| u_k \\right| \\leq u_\\text{max}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d57a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = cp.Parameter(3, name=\"Q\", nonneg=True)\n",
    "R = cp.Parameter(2, name=\"R\", nonneg=True)\n",
    "Q_bar = cp.Parameter(3, name=\"Q_bar\", nonneg=True)\n",
    "R_bar = cp.Parameter(2, name=\"R_bar\", nonneg=True)\n",
    "\n",
    "s_goal = np.array([10.0, 0.0, 0.0])\n",
    "\n",
    "def create_qp_2(nc: int):\n",
    "    objective = cp.Minimize(\n",
    "          0.5 * cp.sum_squares(cp.hstack([cp.diag(Q) @ (mpc_data.statei[i+1] - s_goal) for i in range(nc)]))\n",
    "        + 0.5 * cp.sum_squares(cp.hstack([cp.diag(R) @ (mpc_data.inputi[i]) for i in range(nc)]))\n",
    "        + 0.5 * cp.sum_squares(cp.hstack([cp.diag(Q_bar) @ (mpc_data.statei[i+1] - mpc_data.statebari[i+1].value) for i in range(nc)]))\n",
    "        + 0.5 * cp.sum_squares(cp.hstack([cp.diag(R_bar) @ (mpc_data.inputi[i] - mpc_data.inputbari[i].value) for i in range(nc)]))\n",
    "    )\n",
    "\n",
    "    dynamics_constraints = robot.dt_dynamics_constraint()\n",
    "\n",
    "    input_constraints = [cp.abs(mpc_data.inputi[i][0]) <= v_max for i in range(nc)] + \\\n",
    "        [cp.abs(mpc_data.inputi[i][1]) <= omega_max for i in range(nc)]\n",
    "\n",
    "    constraints = dynamics_constraints + input_constraints\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return problem\n",
    "\n",
    "qp_2_problem = create_qp_2(nc=nc)\n",
    "assert qp_2_problem.is_dpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c85e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_inputs(s0, inputs):\n",
    "    states = [s0]\n",
    "    for u in inputs:\n",
    "        s_next = states[-1] + dt * np.array([\n",
    "            u[0].detach().numpy() * np.cos(states[-1][2]),\n",
    "            u[0].detach().numpy() * np.sin(states[-1][2]),\n",
    "            u[1].detach().numpy(),\n",
    "        ])\n",
    "        states.append(s_next)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_th = torch.nn.Parameter(torch.tensor([10.0, 10.0, 1.0]))\n",
    "R_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "Q_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0, 1.0]))\n",
    "R_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "\n",
    "init_mpc_data(mpc_data)\n",
    "qp_2_problem = create_qp_2(nc=nc)\n",
    "cvxpylayer = CvxpyLayer(\n",
    "    qp_2_problem,\n",
    "    parameters=[Q, R, Q_bar, R_bar, s0],\n",
    "    variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    ")\n",
    "\n",
    "sol = cvxpylayer(Q_th, R_th, Q_bar_th, R_bar_th, s0_th)\n",
    "states = [sol[i] for i in range(nc)]\n",
    "inputs = [sol[nc + i] for i in range(nc)]\n",
    "\n",
    "plot_trajectory(\n",
    "    np.array([s for s in apply_inputs(s0_th.detach().numpy(), inputs)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd322edb",
   "metadata": {},
   "source": [
    "### Loss 1\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\operatorname{RMSE}(u_{\\text{DQP}}, u_{\\text{SQP}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3566256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_dataset_u(inputs_ds, inputs):\n",
    "    return torch.stack([(inputs_ds[i] - inputs[i]).pow(2).sum() for i in range(len(inputs))]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q_th = torch.nn.Parameter(torch.tensor([10.0, 10.0, 1.0]))\n",
    "R_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "Q_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0, 1.0]))\n",
    "R_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "\n",
    "optim_2 = torch.optim.Adam([Q_th, R_th, Q_bar_th, R_bar_th], lr=1e-2)\n",
    "\n",
    "n_epochs = 10\n",
    "history  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1_th.copy_(torch.tensor([1.0]))\n",
    "    w2_th.copy_(torch.tensor([1.0]))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "    \n",
    "    init_mpc_data(mpc_data)\n",
    "    qp_2_problem = create_qp_2(nc=nc)\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        qp_2_problem,\n",
    "        parameters=[Q, R, Q_bar, R_bar, s0],\n",
    "        variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    "    )\n",
    "\n",
    "    sol = cvxpylayer(Q_th, R_th, Q_bar_th, R_bar_th, s0_th)\n",
    "    states = [sol[i] for i in range(nc)]\n",
    "    inputs = [sol[nc + i] for i in range(nc)]\n",
    "    \n",
    "    mpc_data.statebari[0].value = s0_th.detach().cpu().numpy()\n",
    "    for i in range(nc):\n",
    "        mpc_data.statebari[i+1].value = states[i].detach().cpu().numpy()\n",
    "        mpc_data.inputbari[i].value = inputs[i].detach().cpu().numpy()\n",
    "\n",
    "    loss = loss_dataset_u(inputs_ds, inputs)\n",
    "    loss.backward()\n",
    "\n",
    "    optim_2.step()\n",
    "    with torch.no_grad():\n",
    "        Q_th.clamp_(min=1e-6)\n",
    "        R_th.clamp_(min=1e-6)\n",
    "        Q_bar_th.clamp_(min=1e-6)\n",
    "        R_bar_th.clamp_(min=1e-6)\n",
    "\n",
    "    history.append({\n",
    "        'loss': loss.item(),\n",
    "    })\n",
    "    print(f\"Epoch {epoch:2d} | loss = {loss.item():.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot([h['loss'] for h in history])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss Evolution\")\n",
    "\n",
    "plot_trajectory(\n",
    "    np.array([s for s in apply_inputs(s0_th.detach().numpy(), inputs)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a29dc",
   "metadata": {},
   "source": [
    "### Loss 2\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\operatorname{dist}(p_{\\text{goal}}, p_{\\text{final}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_target_distance(s0, inputs):\n",
    "    p_goal_th = torch.tensor(p_goal, requires_grad=True)\n",
    "    \n",
    "    state = s0\n",
    "    for u in inputs:\n",
    "        state = state + dt * torch.stack([\n",
    "            u[0] * torch.cos(state[2]),\n",
    "            u[0] * torch.sin(state[2]),\n",
    "            u[1],\n",
    "        ])\n",
    "\n",
    "    p_final = state[0:2]\n",
    "\n",
    "    return (p_goal_th - p_final).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876440aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_th = torch.nn.Parameter(torch.tensor([10.0, 10.0, 1.0]))\n",
    "R_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "Q_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0, 1.0]))\n",
    "R_bar_th = torch.nn.Parameter(torch.tensor([1.0, 1.0]))\n",
    "s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "\n",
    "optim_2 = torch.optim.Adam([Q_th, R_th, Q_bar_th, R_bar_th], lr=1e-2)\n",
    "\n",
    "n_epochs = 10\n",
    "history  = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    s0_th = torch.tensor([0.0, 0.0, 1.0])\n",
    "    \n",
    "    init_mpc_data(mpc_data)\n",
    "    qp_2_problem = create_qp_2(nc=nc)\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        qp_2_problem,\n",
    "        parameters=[Q, R, Q_bar, R_bar, s0],\n",
    "        variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    "    )\n",
    "\n",
    "    sol = cvxpylayer(Q_th, R_th, Q_bar_th, R_bar_th, s0_th)\n",
    "    states = [sol[i] for i in range(nc)]\n",
    "    inputs = [sol[nc + i] for i in range(nc)]\n",
    "\n",
    "    loss = loss_target_distance(s0_th, inputs)\n",
    "    loss.backward()\n",
    "\n",
    "    optim_2.step()\n",
    "    with torch.no_grad():\n",
    "        Q_th.clamp_(min=1e-6)\n",
    "        R_th.clamp_(min=1e-6)\n",
    "        Q_bar_th.clamp_(min=1e-6)\n",
    "        R_bar_th.clamp_(min=1e-6)\n",
    "\n",
    "    history.append({\n",
    "        'loss': loss.item(),\n",
    "    })\n",
    "    print(f\"Epoch {epoch:2d} | loss = {loss.item():.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot([h['loss'] for h in history])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss Evolution\")\n",
    "\n",
    "plot_trajectory(\n",
    "    np.array([s for s in apply_inputs(s0_th.detach().numpy(), inputs)])\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
