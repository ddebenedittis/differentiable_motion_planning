{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365be60a",
   "metadata": {},
   "source": [
    "# Unicycle Go To Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58622d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "from cycler import cycler\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "from dimp.robots import (\n",
    "    RobotMPCData, UniInput, UniRobot, UniState\n",
    ")\n",
    "\n",
    "from dimp.utils import init_matplotlib, get_colors\n",
    "\n",
    "\n",
    "init_matplotlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56085ce6",
   "metadata": {},
   "source": [
    "### Create The Data for the MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 3      # Number of states (x, y, theta)\n",
    "ni = 2      # Number of inputs (v, omega)\n",
    "\n",
    "nc = 50     # Number of control intervals\n",
    "\n",
    "s0 = cp.Parameter(ns, name=\"s0\")\n",
    "\n",
    "mpc_data = RobotMPCData(\n",
    "    nc=nc,\n",
    "    states_list=[UniState(s0)] + [UniState(cp.Variable(ns, name=f\"s{k+1}\")) for k in range(nc)],\n",
    "    statesbar_list=[UniState(s0)] + [UniState(cp.Parameter(ns)) for _ in range(nc)],\n",
    "    inputs_list=[UniInput(cp.Variable(ni, name=f\"i{k}\")) for k in range(nc)],\n",
    "    inputsbar_list=[UniInput(cp.Parameter(ni)) for _ in range(nc)],\n",
    ")\n",
    "\n",
    "dt = 0.1\n",
    "robot = UniRobot(dt=dt, mpc_data=mpc_data)\n",
    "\n",
    "# Parameters\n",
    "p_goal = np.array([10.0, 0.0])\n",
    "v_max = 10.0\n",
    "omega_max = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b770338",
   "metadata": {},
   "source": [
    "### Create The MPC Problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\substack{s_k, u_k \\\\ \\quad k=1, \\, \\dots, \\, n_c}} \\quad && \\frac{1}{2} \\sum_{k=1}^{n_c} (p_k - p_\\text{goal})^2 + w_1 u_k^2 + w_2 \\left( (s_k - \\bar{s}_k)^2 + (u_k - \\bar{u}_k)^2 \\right) \\\\\n",
    "&\\text{s.t.} && x_{k+1} = A_k x_k + B_k u_k, \\\\\n",
    "& && \\left| u_k \\right| \\leq u_\\text{max}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = cp.Parameter(1, name=\"w1\", nonneg=True)\n",
    "w2 = cp.Parameter(1, name=\"w2\", nonneg=True)\n",
    "\n",
    "def create_qp(nc: int):\n",
    "    objective = cp.Minimize(\n",
    "          0.5 * cp.sum_squares(cp.hstack([mpc_data.statei[i+1][0:2] - p_goal for i in range(nc)]))\n",
    "        + 0.5 * w1 * cp.sum_squares(cp.hstack([mpc_data.inputi[i] for i in range(nc)]))\n",
    "        + 0.5 * w2 * cp.sum_squares(cp.hstack([mpc_data.statei[i+1] - mpc_data.statebari[i+1].value for i in range(nc)]))\n",
    "        + 0.5 * w2 * cp.sum_squares(cp.hstack([mpc_data.inputi[i] - mpc_data.inputbari[i].value for i in range(nc)]))\n",
    "    )\n",
    "\n",
    "    dynamics_constraints = robot.dt_dynamics_constraint()\n",
    "\n",
    "    input_constraints = [cp.abs(mpc_data.inputi[i][0]) <= v_max for i in range(nc)] + \\\n",
    "        [cp.abs(mpc_data.inputi[i][1]) <= omega_max for i in range(nc)]\n",
    "\n",
    "    constraints = dynamics_constraints + input_constraints\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return problem\n",
    "\n",
    "qp_problem = create_qp(nc=nc)\n",
    "assert qp_problem.is_dpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_to(\n",
    "    create_problem: callable,\n",
    "    mpc_data: RobotMPCData,\n",
    "    n_iters: int=1,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate the robot's motion over the OCP horizon (trajectory optimization).\n",
    "    The OCP is solved once and the entire state-input trajectory is obtained in one go.\n",
    "\n",
    "    Args:\n",
    "        problem (cp.Problem): The optimization problem to solve.\n",
    "        mpc_data (RobotMPCData): The MPC data containing state and input variables.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arrays of states and inputs over the simulation steps of size [steps, ns] and [steps, ni], respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    states = np.zeros((mpc_data.nc+1, ns))\n",
    "    inputs = np.zeros((mpc_data.nc, ni))\n",
    "    \n",
    "    mpc_data.statei[0].value = np.array([0.0, 0.0, 1.0])\n",
    "    for i in range(mpc_data.nc):\n",
    "        mpc_data.statei[i+1].value = np.array([0.0, 0.0, 1.0])\n",
    "        mpc_data.inputi[i].value = np.array([0.0, 0.0])\n",
    "    mpc_data.update_bar()\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        problem = create_problem(nc=nc)\n",
    "        problem.solve()\n",
    "        mpc_data.update_bar()\n",
    "    \n",
    "    states[0, :] = mpc_data.statei[0].value\n",
    "    for i in range(mpc_data.nc):\n",
    "        states[i+1, :] = mpc_data.statei[i+1].value\n",
    "        inputs[i, :] = mpc_data.inputi[i].value\n",
    "\n",
    "    return states, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb12a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_mpc(\n",
    "    problem: cp.Problem,\n",
    "    mpc_data: RobotMPCData,\n",
    "    n_steps: int = 200,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate the robot's motion over a fixed number of steps (MPC).\n",
    "    The OCP is solved multiple times and only the first control input is applied at each step.\n",
    "\n",
    "    Args:\n",
    "        problem (cp.Problem): The optimization problem to solve.\n",
    "        mpc_data (RobotMPCData): The MPC data containing state and input variables.\n",
    "        n_steps (int): The number of steps to simulate.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arrays of states and inputs over the simulation steps of size [steps, ns] and [steps, ni], respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    states = np.zeros((n_steps, ns))\n",
    "    inputs = np.zeros((n_steps, ni))\n",
    "\n",
    "    mpc_data.statei[0].value = np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        problem.solve()\n",
    "\n",
    "        mpc_data.statei[0].value = mpc_data.statei[1].value\n",
    "\n",
    "        mpc_data.update_bar()\n",
    "\n",
    "        states[i, :] = mpc_data.statei[1].value\n",
    "        inputs[i, :] = mpc_data.inputi[1].value\n",
    "\n",
    "    return states, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6289a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _triangle_xy(x, y, theta, L=0.6, W=0.35):\n",
    "    pts = np.array([\n",
    "        [ +L/2.0,  0.0      ],  # tip\n",
    "        [ -L/2.0, +W/2.0    ],  # rear-left\n",
    "        [ -L/2.0, -W/2.0    ],  # rear-right\n",
    "        [ +L/2.0,  0.0      ],  # close\n",
    "    ])\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c, -s],[s, c]])\n",
    "    rot = pts @ R.T\n",
    "    rot[:, 0] += x\n",
    "    rot[:, 1] += y\n",
    "    return rot[:, 0], rot[:, 1]\n",
    "\n",
    "def plot_trajectory(states: np.ndarray, body_len=0.6, body_w=0.35) -> None:\n",
    "    # x, y, heading(rad) assumed at [:,0], [:,1], [:,3]\n",
    "    steps = states.shape[0]\n",
    "    thetas = np.unwrap(states[:, 2].astype(float))  # keep radians; unwrap for smooth rotation\n",
    "\n",
    "    xm, xM = states[:, 0].min() - 1, states[:, 0].max() + 1\n",
    "    ym, yM = states[:, 1].min() - 1, states[:, 1].max() + 1\n",
    "\n",
    "    # colors\n",
    "    line_blue = \"rgba(0, 0, 255, 0.85)\"\n",
    "    fill_blue = \"rgba(0, 0, 255, 0.35)\"\n",
    "\n",
    "    # initial triangle\n",
    "    tri_x0, tri_y0 = _triangle_xy(states[0, 0], states[0, 1], thetas[0], body_len, body_w)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            # 0) trajectory (blue)\n",
    "            go.Scatter(\n",
    "                x=states[:, 0], y=states[:, 1],\n",
    "                mode=\"lines\", name=\"Trajectory\",\n",
    "                line=dict(width=2, color=\"rgba(0, 0, 255, 0.5)\", dash=\"dot\")\n",
    "            ),\n",
    "            # 1) vehicle triangle (animated trace) — blue\n",
    "            go.Scatter(\n",
    "                x=tri_x0, y=tri_y0,\n",
    "                mode=\"lines\", fill=\"toself\",\n",
    "                line=dict(width=1, color=line_blue),\n",
    "                fillcolor=fill_blue,\n",
    "                opacity=1.0,\n",
    "                hoverinfo=\"skip\",\n",
    "                showlegend=False,            # hide this from legend…\n",
    "                name=\"Vehicle\"\n",
    "            ),\n",
    "            # 2) legend-only proxy with a triangle icon\n",
    "            go.Scatter(\n",
    "                x=[states[0, 0]], y=[states[0, 1]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(symbol=\"triangle-right\", size=12, color=line_blue),\n",
    "                name=\"Vehicle\",\n",
    "                visible=\"legendonly\",        # …and show this only in the legend\n",
    "                hoverinfo=\"skip\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # layout + slow animation\n",
    "    fig.update_layout(\n",
    "        width=600, height=450,\n",
    "        xaxis=dict(range=[xm, xM], autorange=False, zeroline=False, scaleanchor=\"y\", title=\"x [m]\"),\n",
    "        yaxis=dict(range=[ym, yM], autorange=False, zeroline=False, title=\"y [m]\"),\n",
    "        title_text=\"Trajectory\", title_x=0.5,\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[dict(\n",
    "                args=[None, {\n",
    "                    \"frame\": {\"duration\": 100, \"redraw\": False},  # slower\n",
    "                    \"fromcurrent\": True,\n",
    "                    \"transition\": {\"duration\": 0},\n",
    "                    \"mode\": \"immediate\"\n",
    "                }],\n",
    "                label=\"Play\",\n",
    "                method=\"animate\",\n",
    "            )]\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    # frames: update only the triangle trace (index 1)\n",
    "    fig.update(frames=[\n",
    "        go.Frame(\n",
    "            data=[go.Scatter(*(), x=_triangle_xy(states[k, 0], states[k, 1], thetas[k], body_len, body_w)[0],\n",
    "                                  y=_triangle_xy(states[k, 0], states[k, 1], thetas[k], body_len, body_w)[1])],\n",
    "            traces=[1]\n",
    "        ) for k in range(steps)\n",
    "    ])\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(\"omni_robot_mpc.html\", include_plotlyjs=\"cdn\", full_html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_plot_qp():\n",
    "    w1.value = np.array([1.0])\n",
    "    w2.value = np.array([0.1])\n",
    "    \n",
    "    for n_iters in range(5):\n",
    "        states, inputs = simulate_to(create_qp, mpc_data, n_iters=n_iters+1)\n",
    "        \n",
    "        plot_trajectory(states)\n",
    "\n",
    "simulate_and_plot_qp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51a3af",
   "metadata": {},
   "source": [
    "### Create the `CvxpyLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75567e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_loss(states, inputs):\n",
    "    p_goal_th = torch.tensor(p_goal, requires_grad=True)\n",
    "    \n",
    "    st_cost = torch.stack([(s[0:2] - p_goal_th).pow(2).sum() for s in states]).sum()\n",
    "    in_cost = torch.stack([u.pow(2).sum()               for u in inputs]).sum()\n",
    "    return 0.5 * st_cost + 0.5 * 0.1 * in_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e33176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_res(history):\n",
    "    colors = get_colors()\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot([h['loss'] for h in history])\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_title(\"Loss Evolution\")\n",
    "\n",
    "    ax11 = ax[1].twinx()\n",
    "    ax[1].plot([h['w'][0] for h in history],)\n",
    "    ax[1].set_title(\"Weights Evolution\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(r\"$w_0$\", color=colors[0])\n",
    "    ax[1].tick_params(axis='y', labelcolor=colors[0])\n",
    "\n",
    "    ax11.plot([h['w'] for h in history], color=colors[1], linestyle='--')\n",
    "    ax11.set_ylabel(r\"$w_1$\", color=colors[1])\n",
    "    ax11.tick_params(axis='y', labelcolor=colors[1])\n",
    "\n",
    "    fig.set_constrained_layout(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvxpylayer = CvxpyLayer(\n",
    "    qp_problem,\n",
    "    parameters=[w1, w2, s0],\n",
    "    variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    ")\n",
    "\n",
    "w1_th = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "w2_th = torch.tensor([1.0])\n",
    "s0_th = torch.tensor([0.0, 0.0, 0.1])\n",
    "\n",
    "optim = torch.optim.Adam([w1_th], lr=5e-3)\n",
    "\n",
    "solution = cvxpylayer(w1_th, w2_th, s0_th)\n",
    "\n",
    "solution[0].sum().backward()\n",
    "\n",
    "print(f\"w1.grad: {w1_th.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e52fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "history  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1_th.copy_(torch.tensor([1.0]))\n",
    "    w2_th.copy_(torch.tensor([1.0]))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    s0_th = torch.zeros_like(s0_th)\n",
    "    \n",
    "    qp_problem = create_qp(nc=nc)\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        qp_problem,\n",
    "        parameters=[w1, w2, s0],\n",
    "        variables=[mpc_data.statei[i] for i in range(1, nc + 1)] + [mpc_data.inputi[i] for i in range(nc)],\n",
    "    )\n",
    "\n",
    "    sol = cvxpylayer(w1_th, w2_th, s0_th)\n",
    "    states = [sol[i] for i in range(nc)]\n",
    "    inputs = [sol[nc + i] for i in range(nc)]\n",
    "    \n",
    "    mpc_data.statebari[0].value = s0_th.detach().cpu().numpy()\n",
    "    for i in range(nc):\n",
    "        mpc_data.statebari[i+1].value = states[i].detach().cpu().numpy()\n",
    "        mpc_data.inputbari[i].value = inputs[i].detach().cpu().numpy()\n",
    "\n",
    "    loss = task_loss(states, inputs)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    with torch.no_grad():\n",
    "        w1_th.clamp_(min=1e-6)\n",
    "\n",
    "    history.append({\n",
    "        'loss': loss.item(),\n",
    "        'w': copy.deepcopy(w1_th.detach().cpu().numpy()),\n",
    "        'dw': copy.deepcopy(w1_th.grad.detach().cpu().numpy()),\n",
    "    })\n",
    "    print(f\"Epoch {epoch:2d} | loss = {loss.item():.4f} | w = {w1_th.detach().cpu().numpy()}\")\n",
    "    \n",
    "plot_training_res(history)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
