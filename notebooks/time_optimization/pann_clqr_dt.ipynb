{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dimp.utils import init_matplotlib, get_colors\n",
    "\n",
    "from utils import (\n",
    "    RunMode, get_n_epochs, get_method_run_mode, pickle_name,\n",
    "    theta_2_dt, Ad_Bd_from_dt, LQs_LRs_from_dt,\n",
    "    zoh_cost_matrices,\n",
    "    task_loss, uniform_resampling_loss, substep_loss, evaluate_continuous_cost,\n",
    "    plot_timegrid, plot_colored, plot_training_res, save_training_res,\n",
    "    save_pickle, load_pickle,\n",
    "    extract_trajectory_data, compute_trajectory_metrics,\n",
    "    plot_density_and_changes, plot_cross_correlations,\n",
    ")\n",
    "from pann_clqr import (\n",
    "    create_pann_clqr,\n",
    "    create_pann_param_clqr,\n",
    "    create_pann_param_clqr_2,\n",
    "    create_exact_param_pann_clqr,\n",
    "    create_exact_param_pann_clqr_2,\n",
    "    create_exact_zoh_cost_clqr,\n",
    ")\n",
    "\n",
    "init_matplotlib()\n",
    "colors = get_colors()\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2825869",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mode = RunMode.DISPLAY      # DISPLAY | TEST | FULL (default for all methods)\n",
    "run_overrides = {               # Per-method overrides (empty = use run_mode for all)\n",
    "    # \"zoh3\": RunMode.FULL,     # Uncomment to rerun only ZOH3\n",
    "}\n",
    "\n",
    "n = 160                         # Number of timesteps (default, used by most methods)\n",
    "n_zoh3 = 80                     # Number of timesteps for ZOH3\n",
    "\n",
    "n_epochs_override = None        # Set to int to override all methods' epoch counts\n",
    "out_dir = \"data/pann_clqr_dt\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e9dbe",
   "metadata": {},
   "source": [
    "## Example Pannocchia ðŸŒ½\n",
    "\n",
    "Contnuous LTI system with dynamics\n",
    "$$\n",
    "\\dot{s} = A s + B u\n",
    "$$\n",
    "\n",
    "3 states and 1 input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b31ff",
   "metadata": {},
   "source": [
    "### Define the problem matrices, the initial state, the time horizon, and the LQR matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [-0.1, 0, 0],\n",
    "    [0, -2, -6.25],\n",
    "    [0, 4, 0]\n",
    "])\n",
    "\n",
    "B = np.array([[0.25], [2.0], [0.0]])\n",
    "\n",
    "s0 = np.array([1.344, -4.585, 5.674])   # initial state\n",
    "\n",
    "T = 10.0        # time window\n",
    "\n",
    "N = 1000        # max number of timesteps of the OCP\n",
    "\n",
    "Q = 1.0 * np.eye(3)\n",
    "R = 0.1 * np.eye(1)\n",
    "\n",
    "u_max = 1.0     # max control input\n",
    "\n",
    "n_s = 3         # number of states\n",
    "n_u = 1         # number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b83f5b",
   "metadata": {},
   "source": [
    "### Create the Optimization Problem\n",
    "\n",
    "Classic constrained LQR problem.\n",
    "Discretized with first-order Euler.\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left( s_k^T Q s_k + u_k^T R u_k \\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = s_k + \\Delta t (A s_k + B u_k), \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d964ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_base = T / N\n",
    "pann_clqr, s_base, u_base = create_pann_clqr(N, s0, A, B, Q, R, dt_base, u_max)\n",
    "assert pann_clqr.is_dpp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53c89b",
   "metadata": {},
   "source": [
    "### Study how the solution changes with different number of samples.\n",
    "\n",
    "With $n=80$ samples the system is unstable.\n",
    "From $n=160$ timesteps the OCP stabilizes the system.\n",
    "However, increasing the number of timesteps further improves the solution (see the optimal cost value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc12437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of timesteps of the trajectory optimization problem.\n",
    "interval = 80\n",
    "\n",
    "fig, axs = plt.subplots(12, 2, figsize=(6.4, 12.8), constrained_layout=True)\n",
    "\n",
    "i = 0\n",
    "for n_test in range(interval, N + 1, interval):\n",
    "    dt_test = T / n_test\n",
    "    prob_test, s_test, u_test = create_pann_clqr(n_test, s0, A, B, Q, R, dt_test, u_max)\n",
    "\n",
    "    start_time = time.time()\n",
    "    prob_test.solve()\n",
    "    solve_time = time.time() - start_time\n",
    "    \n",
    "    print(\n",
    "        f\"n = {n_test}\\n\"\n",
    "        f\"Optimal cost: {prob_test.objective.value:.4f}, \"\n",
    "        f\"solve time meas: {solve_time:.4f} s, \"\n",
    "        f\"solve time: {prob_test.solver_stats.solve_time:.4f} s\\n\"\n",
    "    )\n",
    "\n",
    "    times = np.arange(n_test) * dt_test\n",
    "    s_vec = np.array([sv.value for sv in s_test[1:n_test+1]])\n",
    "    u_vec = np.array([uv.value for uv in u_test[:n_test]])\n",
    "\n",
    "    axs[2*(i//2), i%2].plot(times, s_vec, label=['x', 'y', 'z'])\n",
    "    axs[2*(i//2), i%2].set(\n",
    "        xlabel='Time',\n",
    "        ylabel='State',\n",
    "        title=fr\"$n={n_test}$\",\n",
    "    )\n",
    "\n",
    "    axs[2*(i//2)+1, i%2].plot(times, u_vec)\n",
    "    axs[2*(i//2)+1, i%2].set(\n",
    "        xlabel='Time',\n",
    "        ylabel='Input',\n",
    "    )\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521047f",
   "metadata": {},
   "source": [
    "## DQP with Auxiliary Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f025161",
   "metadata": {},
   "source": [
    "### Create the Parametrized CLQR Problem\n",
    "\n",
    "Discretize then linearize the (nonlinear) dynamics.\n",
    "$$\n",
    "s_{k+1} = \\bar{s}_k + \\bar{\\delta}_k (A \\bar{s}_k + B u_k) + (I + \\bar{\\delta}_k A) \\tilde{s}_k + \\bar{\\delta}_k B u_k + (A \\bar{s}_k + B u_k) \\tilde{\\delta}_k,\n",
    "$$\n",
    "where $\\tilde{\\square} = \\square - \\bar{\\square}$ and $\\delta_k$ is an auxiliary variable that represents the time step lengths.\n",
    "\n",
    "The optimization vector of the QP is $\\begin{bmatrix} \\tilde{s}_{k=1,\\dots,N} & \\tilde{u}_{k=0,\\dots,N-1} & \\tilde{\\delta}_{k=0,\\dots,N} -1\\end{bmatrix}$.\n",
    "\n",
    "The QP is parametrized by the parameter vector $\\bar{\\delta} = \\begin{bmatrix} \\bar{\\delta}_1 & \\dots & \\bar{\\delta}_N \\end{bmatrix}^T$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k, \\delta_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left(\\bar{\\delta}_k \\left( s_k^T Q s_k + u_k^T R u_k \\right) + w_\\delta \\tilde{\\delta}_k^2 \\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = \\bar{s}_k + \\bar{\\delta}_k (A \\bar{s}_k + B \\bar{u}_k) + (I + \\bar{\\delta}_k A) \\tilde{s}_k + \\bar{\\delta}_k B \\tilde{u}_k + (A \\bar{s}_k + B \\bar{u}_k) \\tilde{\\delta}_k, \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a01570",
   "metadata": {},
   "source": [
    "### Task Loss\n",
    "\n",
    "| Method          | Loss                                                                                                           |\n",
    "| --------------- | -------------------------------------------------------------------------------------------------------------- |\n",
    "| Unscaled        | $$\\mathcal{L}_1 = \\sum_{k=0}^{N} \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)$$                |\n",
    "| Time scaled     | $$\\mathcal{L}_2 = \\sum_{k=0}^{N} \\delta_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)$$       |\n",
    "| Time bar scaled | $$\\mathcal{L}_3 = \\sum_{k=0}^{N} \\bar{\\delta}_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417712b8",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1da04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods = [\"time scaled\", \"time bar scaled\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"aux\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"aux\", n_epochs_override)\n",
    "\n",
    "    history_aux = []\n",
    "    sol_aux = {}\n",
    "\n",
    "    for method in loss_methods:\n",
    "        print(f\"Method: {method}\")\n",
    "\n",
    "        dt_init = T / n\n",
    "        dts_torch = [torch.nn.Parameter(torch.ones(1) * dt_init) for _ in range(n)]\n",
    "        optim = torch.optim.Adam(dts_torch, lr=5e-4)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for d in dts_torch:\n",
    "                d.copy_(torch.ones(1) * dt_init)\n",
    "\n",
    "        s_bar = [s0 + (np.zeros(n_s) - s0) * i / n for i in range(n)]\n",
    "        u_bar = [np.zeros(n_u) for _ in range(n)]\n",
    "\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad()\n",
    "\n",
    "                _, layer_aux, _, _, _, dts_params = create_pann_param_clqr(\n",
    "                    n, s0, A, B, Q, R, s_bar, u_bar, u_max, T,\n",
    "                )\n",
    "                sol_aux[method] = layer_aux(*dts_torch)\n",
    "\n",
    "                s_bar = [sol_aux[method][i].detach().numpy() for i in range(n)]\n",
    "                u_bar = [sol_aux[method][n + i].detach().numpy() for i in range(n)]\n",
    "\n",
    "                # Aux-specific loss: extract deltas from solution\n",
    "                states_sol = [sol_aux[method][i] for i in range(n)]\n",
    "                inputs_sol = [sol_aux[method][n + i] for i in range(n)]\n",
    "                deltas_sol = [sol_aux[method][2 * n + i] for i in range(n)]\n",
    "\n",
    "                if method == \"time scaled\":\n",
    "                    loss = sum(\n",
    "                        deltas_sol[i] * states_sol[i].t() @ torch.tensor(Q, dtype=torch.float32) @ states_sol[i]\n",
    "                        for i in range(n)\n",
    "                    ) + sum(\n",
    "                        deltas_sol[i] * inputs_sol[i].t() @ torch.tensor(R, dtype=torch.float32) @ inputs_sol[i]\n",
    "                        for i in range(n)\n",
    "                    )\n",
    "                elif method == \"time bar scaled\":\n",
    "                    deltas_bar = np.concatenate([d.detach().numpy() for d in dts_torch])\n",
    "                    loss = sum(\n",
    "                        deltas_bar[i] * states_sol[i].t() @ torch.tensor(Q, dtype=torch.float32) @ states_sol[i]\n",
    "                        for i in range(n)\n",
    "                    ) + sum(\n",
    "                        deltas_bar[i] * inputs_sol[i].t() @ torch.tensor(R, dtype=torch.float32) @ inputs_sol[i]\n",
    "                        for i in range(n)\n",
    "                    )\n",
    "                else:\n",
    "                    loss = task_loss(states_sol, inputs_sol, dts_torch, Q, R, method=\"unscaled\")\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for d in dts_torch:\n",
    "                        d.clamp_(min=1e-6, max=0.07)\n",
    "                        d *= T / sum(dts_torch)\n",
    "\n",
    "                history_aux.append({\n",
    "                    'method': method,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': [d.detach().numpy() for d in dts_torch],\n",
    "                })\n",
    "\n",
    "    save_pickle(out_dir, \"sol_aux\", sol_aux)\n",
    "    save_pickle(out_dir, \"history_aux\", history_aux)\n",
    "else:\n",
    "    sol_aux = load_pickle(out_dir, \"sol_aux\")\n",
    "    history_aux = load_pickle(out_dir, \"history_aux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in loss_methods:\n",
    "    hist_m = [h for h in history_aux if h['method'] == method]\n",
    "    plot_training_res(sol_aux[method], hist_m, n, sol_method=1)\n",
    "    save_training_res(\n",
    "        \"out\", method.replace(\" \", \"_\"),\n",
    "        sol_aux[method], hist_m, n, sol_method=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c1f19",
   "metadata": {},
   "source": [
    "## DQP with Reparametrized Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6da6c",
   "metadata": {},
   "source": [
    "### DQP With Reparametrized Timesteps\n",
    "\n",
    "Reparametrize time steps with the simplex:\n",
    "$$\n",
    "\\Delta t_k = \\epsilon + (T - n \\epsilon) \\frac{e^{\\theta_k}}{\\sum_j e^{\\theta_j}}\n",
    "$$\n",
    "This enforces both positivity and the total time constraint $\\sum_k \\Delta t_k = T$ without discontinuous updates.\n",
    "\n",
    "The optimization vector of the QP is $\\begin{bmatrix} \\tilde{s}_{k=1,\\dots,N} & \\tilde{u}_{k=0,\\dots,N-1}\\end{bmatrix}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left(\\Delta t_k \\left( s_k^T Q s_k + u_k^T R u_k \\right)\\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = \\bar{s}_k + \\Delta t_k (A \\bar{s}_k + B \\bar{u}_k) + (I + \\Delta t_k A) \\tilde{s}_k + \\Delta t_k B \\tilde{u}_k, \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The OCP uses $\\Delta t_k (\\theta)$ as a parameter. The optimizer optimizes $\\theta$. Gradients do flow through the softmax function in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c778f",
   "metadata": {},
   "source": [
    "### Task Loss\n",
    "\n",
    "| Method       | Loss                                                                                                       |\n",
    "| ------------ | ---------------------------------------------------------------------------------------------------------- |\n",
    "| ~~Unscaled~~ | $$\\mathcal{L}_1 = \\sum_{k=0}^{N} \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)$$            |\n",
    "| Time scaled  | $$\\mathcal{L}_2 = \\sum_{k=0}^{N} \\Delta t_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)$$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_2 = [\"time scaled\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"rep\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"rep\", n_epochs_override)\n",
    "\n",
    "    history_rep = []\n",
    "    sol_rep = {}\n",
    "\n",
    "    for method in loss_methods_2:\n",
    "        print(f\"Method: {method}\")\n",
    "\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        _, layer_rep, _, _, _ = create_pann_param_clqr_2(n, s0, A, B, Q, R, u_max)\n",
    "\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                dts_torch = theta_2_dt(theta, T, n)\n",
    "                sol_rep[method] = layer_rep(dts_torch)\n",
    "\n",
    "                states_sol = [sol_rep[method][i] for i in range(n)]\n",
    "                inputs_sol = [sol_rep[method][n + i] for i in range(n)]\n",
    "\n",
    "                loss = task_loss(states_sol, inputs_sol, dts_torch, Q, R,\n",
    "                                method=\"time_scaled\" if method == \"time scaled\" else \"unscaled\")\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                history_rep.append({\n",
    "                    'method': method,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': dts_torch.detach().numpy(),\n",
    "                })\n",
    "\n",
    "    save_pickle(out_dir, \"history_rep\", history_rep)\n",
    "    save_pickle(out_dir, \"sol_rep\", sol_rep)\n",
    "else:\n",
    "    history_rep = load_pickle(out_dir, \"history_rep\")\n",
    "    sol_rep = load_pickle(out_dir, \"sol_rep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23326406",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in loss_methods_2:\n",
    "    hist_m = [h for h in history_rep if h['method'] == method]\n",
    "    plot_training_res(sol_rep[method], hist_m, n, sol_method=2)\n",
    "    save_training_res(\n",
    "        \"out\", method.replace(\" \", \"_\") + \"_rep\",\n",
    "        sol_rep[method], hist_m, n, sol_method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d5733",
   "metadata": {},
   "source": [
    "## Loss Hyper Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8de8c5",
   "metadata": {},
   "source": [
    "### Gradient Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_flow():\n",
    "    \"\"\"Verify gradients flow correctly through both loss functions.\"\"\"\n",
    "    n_test = 20\n",
    "    theta_test = torch.nn.Parameter(torch.ones(n_test, 1))\n",
    "\n",
    "    A_torch = torch.tensor(A, dtype=torch.float32)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_torch = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_torch = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_torch = torch.tensor(s0, dtype=torch.float32)\n",
    "\n",
    "    dts_test = theta_2_dt(theta_test, T, n_test)\n",
    "    inputs_test = [torch.randn(n_u, requires_grad=True) for _ in range(n_test)]\n",
    "\n",
    "    # === Uniform Resampling ===\n",
    "    loss1 = uniform_resampling_loss(\n",
    "        inputs_test, dts_test, s0_torch,\n",
    "        A_torch, B_torch, Q_torch, R_torch,\n",
    "        T=T, n_res=100, use_exact=False,\n",
    "    )\n",
    "    loss1.backward()\n",
    "\n",
    "    print(\"Approach 1 (Uniform Resampling - Euler):\")\n",
    "    print(f\"  Loss value: {loss1.item():.6f}\")\n",
    "    print(f\"  theta.grad exists: {theta_test.grad is not None}\")\n",
    "    if theta_test.grad is not None:\n",
    "        print(f\"  theta.grad norm: {theta_test.grad.norm().item():.6f}\")\n",
    "\n",
    "    theta_test.grad = None\n",
    "    for u in inputs_test:\n",
    "        if u.grad is not None:\n",
    "            u.grad = None\n",
    "\n",
    "    dts_test = theta_2_dt(theta_test, T, n_test)\n",
    "\n",
    "    # === Substeps ===\n",
    "    loss2 = substep_loss(\n",
    "        inputs_test, dts_test, s0_torch,\n",
    "        A_torch, B_torch, Q_torch, R_torch,\n",
    "        n_sub=10, use_exact=False,\n",
    "    )\n",
    "    loss2.backward()\n",
    "\n",
    "    print(\"\\nApproach 2 (Substeps - Euler):\")\n",
    "    print(f\"  Loss value: {loss2.item():.6f}\")\n",
    "    print(f\"  theta.grad exists: {theta_test.grad is not None}\")\n",
    "    if theta_test.grad is not None:\n",
    "        print(f\"  theta.grad norm: {theta_test.grad.norm().item():.6f}\")\n",
    "\n",
    "test_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a20d5",
   "metadata": {},
   "source": [
    "### Training with Hyper-Sampling (HS) Losses\n",
    "\n",
    "Problem that HS attempts to solve: the loss is computed only along the trajectory (non-uniform) sampling points. The loss may not reflect the true cost over the entire time horizon.\n",
    "\n",
    "A possible solution would be to compute the exact cost along the trajectory assuming ZOH inputs. Instead, here, we propose two different strategies.\n",
    "\n",
    "#### Dense Uniform Grid\n",
    "\n",
    "The non-uniform grid is replaced with a dense uniform grid. The input is constant within the original non-uniform intervals. The dynamics is integrated with first-order Euler on the dense grid.\n",
    "\n",
    "**Main problem**: the dense grid is computed with `detached()`, the gradients do not flow through it.\n",
    "\n",
    "#### Substeps\n",
    "\n",
    "Within each original non-uniform interval, we introduce $M$ substeps. The dynamics is integrated with first-order Euler on the substeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_hs = [\"uniform_resample\", \"substeps\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"hs\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"hs\", n_epochs_override)\n",
    "    n_res_hs = 1000\n",
    "    n_sub_hs = 10\n",
    "\n",
    "    history_hs = []\n",
    "    sol_hs = {}\n",
    "\n",
    "    A_torch = torch.tensor(A, dtype=torch.float32)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_torch = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_torch = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_torch = torch.tensor(s0, dtype=torch.float32)\n",
    "\n",
    "    for method in loss_methods_hs:\n",
    "        print(f\"Training with method: {method}\")\n",
    "\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        _, layer_hs, _, _, _ = create_pann_param_clqr_2(n, s0, A, B, Q, R, u_max)\n",
    "\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                dts_torch = theta_2_dt(theta, T, n)\n",
    "                sol_hs[method] = layer_hs(dts_torch)\n",
    "\n",
    "                inputs_qp = [sol_hs[method][n + i] for i in range(n)]\n",
    "\n",
    "                if method == \"uniform_resample\":\n",
    "                    loss = uniform_resampling_loss(\n",
    "                        inputs_qp, dts_torch, s0_torch,\n",
    "                        A_torch, B_torch, Q_torch, R_torch,\n",
    "                        T=T, n_res=n_res_hs, use_exact=False,\n",
    "                    )\n",
    "                elif method == \"substeps\":\n",
    "                    loss = substep_loss(\n",
    "                        inputs_qp, dts_torch, s0_torch,\n",
    "                        A_torch, B_torch, Q_torch, R_torch,\n",
    "                        n_sub=n_sub_hs, use_exact=False,\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                history_hs.append({\n",
    "                    'method': method,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': dts_torch.detach().cpu().numpy(),\n",
    "                })\n",
    "\n",
    "        print(f\"  Final loss: {history_hs[-1]['loss']:.6f}\\n\")\n",
    "\n",
    "    save_pickle(out_dir, \"sol_hs\", sol_hs)\n",
    "    save_pickle(out_dir, \"history_hs\", history_hs)\n",
    "else:\n",
    "    sol_hs = load_pickle(out_dir, \"sol_hs\")\n",
    "    history_hs = load_pickle(out_dir, \"history_hs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dfdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in loss_methods_hs:\n",
    "    hist_m = [h for h in history_hs if h['method'] == method]\n",
    "    plot_training_res(sol_hs[method], hist_m, n, sol_method=2)\n",
    "    save_training_res(\n",
    "        \"out\", method,\n",
    "        sol_hs[method], hist_m, n, sol_method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot: uniform resampling vs substeps\n",
    "fig, axs = plt.subplots(2, 2, figsize=(9.6, 6.4), constrained_layout=True)\n",
    "\n",
    "for method in loss_methods_hs:\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    axs[0, 0].plot([h['loss'] for h in history_method], label=method)\n",
    "axs[0, 0].set_xlabel(\"Epoch\")\n",
    "axs[0, 0].set_ylabel(\"Loss\")\n",
    "axs[0, 0].set_title(\"Loss Convergence\")\n",
    "axs[0, 0].legend()\n",
    "\n",
    "for i, method in enumerate(loss_methods_hs):\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    d_arr = history_method[-1]['dts']\n",
    "    times = np.cumsum(d_arr)\n",
    "    axs[0, 1].plot(times, d_arr, label=method)\n",
    "axs[0, 1].set_xlabel(\"Time\")\n",
    "axs[0, 1].set_ylabel(\"Timestep duration\")\n",
    "axs[0, 1].set_title(\"Final Timestep Distributions\")\n",
    "axs[0, 1].legend()\n",
    "\n",
    "for i, method in enumerate(loss_methods_hs):\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    d_arr = history_method[-1]['dts']\n",
    "    axs[1, i].hist(d_arr.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "    axs[1, i].set_xlabel(\"Timestep duration\")\n",
    "    axs[1, i].set_ylabel(\"Count\")\n",
    "    axs[1, i].set_title(f\"Histogram: {method}\")\n",
    "    axs[1, i].axvline(T/n, color='r', linestyle='--', label=f\"uniform={T/n:.4f}\")\n",
    "    axs[1, i].legend()\n",
    "\n",
    "fig.suptitle(\"Comparison: Uniform Resampling vs Substeps\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \"True\" Continuous-Time Cost\n",
    "print(\"=== True Continuous-Time Cost Comparison ===\\n\")\n",
    "\n",
    "for method in loss_methods_hs:\n",
    "    sol = sol_hs[method]\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    dts_final = history_method[-1]['dts']\n",
    "    inputs_qp = [sol[n + i] for i in range(n)]\n",
    "\n",
    "    true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "\n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Training loss (final): {history_method[-1]['loss']:.4f}\")\n",
    "    print(f\"  True continuous cost:  {true_cost:.4f}\")\n",
    "    print(f\"  dt range: [{np.min(dts_final):.5f}, {np.max(dts_final):.5f}]\")\n",
    "    print(f\"  dt std:   {np.std(dts_final):.5f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Comparison with Other Methods ===\\n\")\n",
    "\n",
    "try:\n",
    "    for method in loss_methods_2:\n",
    "        sol = sol_rep[method]\n",
    "        history_method = [h for h in history_rep if h['method'] == method]\n",
    "        dts_final = history_method[-1]['dts']\n",
    "        inputs_qp = [sol[n + i] for i in range(n)]\n",
    "        true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "        print(f\"Reparametrized ({method}): true_cost = {true_cost:.4f}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for method in loss_methods_zoh:\n",
    "        sol = sol_zoh[method]\n",
    "        history_method = [h for h in history_zoh if h['method'] == method]\n",
    "        dts_final = history_method[-1]['dts']\n",
    "        inputs_qp = [sol[n + i] for i in range(n)]\n",
    "        true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "        print(f\"ZOH ({method}): true_cost = {true_cost:.4f}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1bba9",
   "metadata": {},
   "source": [
    "## DQP With ZOH Exact Discretization\n",
    "\n",
    "Use the exact discretization of the LTI system with zero-order hold (ZOH).\n",
    "\n",
    "OCP parameters:\n",
    "$$\n",
    "A_{d, k}, B_{d, k}\n",
    "= \\operatorname{ZOH}(A, B, \\Delta t_k)\n",
    "= \\exp \\left( \\begin{bmatrix} A & B \\\\ 0 & 0 \\end{bmatrix} \\Delta t_k \\right)\n",
    "= \\begin{bmatrix} A_{d, k} & B_{d, k} \\\\ 0 & I \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21531eb4",
   "metadata": {},
   "source": [
    "### No Cost Scaling With Interval Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e553c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh = [\"time scaled\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"zoh\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"zoh\", n_epochs_override)\n",
    "    history_zoh = []\n",
    "    sol_zoh = {}\n",
    "\n",
    "    for method in loss_methods_zoh:\n",
    "        print(f\"ZOH Method: {method}\")\n",
    "\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        _, layer_zoh, _, _, _, _ = create_exact_param_pann_clqr(\n",
    "            n, s0, n_s, n_u, Q, R, u_max,\n",
    "        )\n",
    "\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                dts_torch = theta_2_dt(theta, T, n)\n",
    "\n",
    "                Ad_list, Bd_list = zip(*[Ad_Bd_from_dt(dt_k, A, B) for dt_k in dts_torch])\n",
    "\n",
    "                sol_zoh[method] = layer_zoh(*Ad_list, *Bd_list)\n",
    "\n",
    "                states_sol = [sol_zoh[method][i] for i in range(n)]\n",
    "                inputs_sol = [sol_zoh[method][n + i] for i in range(n)]\n",
    "                loss = task_loss(states_sol, inputs_sol, dts_torch, Q, R,\n",
    "                                method=\"time_scaled\" if method == \"time scaled\" else \"unscaled\")\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                history_zoh.append({\n",
    "                    \"method\": method,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": float(loss.item()),\n",
    "                    \"dts\": dts_torch.detach().cpu().numpy(),\n",
    "                })\n",
    "\n",
    "    save_pickle(out_dir, \"sol_zoh\", sol_zoh)\n",
    "    save_pickle(out_dir, \"history_zoh\", history_zoh)\n",
    "else:\n",
    "    sol_zoh = load_pickle(out_dir, \"sol_zoh\")\n",
    "    history_zoh = load_pickle(out_dir, \"history_zoh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62536cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in loss_methods_zoh:\n",
    "    hist_m = [h for h in history_zoh if h['method'] == method]\n",
    "    plot_training_res(sol_zoh[method], hist_m, n, sol_method=2)\n",
    "    save_training_res(\n",
    "        \"out\", method.replace(\" \", \"_\") + \"_zoh\",\n",
    "        sol_zoh[method], hist_m, n, sol_method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aceea3",
   "metadata": {},
   "source": [
    "### With Cost Scaling With Interval Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b14cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh_2 = [\"time scaled\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"zoh2\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"zoh2\", n_epochs_override)\n",
    "    history_zoh_2 = []\n",
    "    sol_zoh_2 = {}\n",
    "\n",
    "    for method in loss_methods_zoh_2:\n",
    "        print(f\"ZOH2 Method: {method}\")\n",
    "\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        _, layer_zoh2, _, _, _, _, _, _ = create_exact_param_pann_clqr_2(\n",
    "            n, s0, n_s, n_u, u_max,\n",
    "        )\n",
    "\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                dts_torch = theta_2_dt(theta, T, n)\n",
    "\n",
    "                Ad_list, Bd_list = zip(*[Ad_Bd_from_dt(dt_k, A, B) for dt_k in dts_torch])\n",
    "                LQs_list, LRs_list = LQs_LRs_from_dt(dts_torch, Q, R)\n",
    "\n",
    "                sol_zoh_2[method] = layer_zoh2(*Ad_list, *Bd_list, *LQs_list, *LRs_list)\n",
    "\n",
    "                states_sol = [sol_zoh_2[method][i] for i in range(n)]\n",
    "                inputs_sol = [sol_zoh_2[method][n + i] for i in range(n)]\n",
    "                loss = task_loss(states_sol, inputs_sol, dts_torch, Q, R,\n",
    "                                method=\"time_scaled\" if method == \"time scaled\" else \"unscaled\")\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                history_zoh_2.append({\n",
    "                    \"method\": method,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": float(loss.item()),\n",
    "                    \"dts\": dts_torch.detach().cpu().numpy(),\n",
    "                })\n",
    "\n",
    "    save_pickle(out_dir, \"sol_zoh_2\", sol_zoh_2)\n",
    "    save_pickle(out_dir, \"history_zoh_2\", history_zoh_2)\n",
    "else:\n",
    "    sol_zoh_2 = load_pickle(out_dir, \"sol_zoh_2\")\n",
    "    history_zoh_2 = load_pickle(out_dir, \"history_zoh_2\")\n",
    "\n",
    "for method in loss_methods_zoh_2:\n",
    "    hist_m = [h for h in history_zoh_2 if h['method'] == method]\n",
    "    plot_training_res(sol_zoh_2[method], hist_m, n, sol_method=2)\n",
    "    save_training_res(\n",
    "        \"out\", method.replace(\" \", \"_\") + \"_zoh_2\",\n",
    "        sol_zoh_2[method], hist_m, n, sol_method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65935b99",
   "metadata": {},
   "source": [
    "## DQP with Exact ZOH Discretization and Exact Integrated Cost\n",
    "\n",
    "This section implements the approach from Pannocchia et al.: **exact ZOH discretization** of both dynamics and quadratic stage cost, with all nonlinear dependence on $\\Delta t_k$ computed outside the QP.\n",
    "\n",
    "### Exact ZOH Integrated Stage Cost\n",
    "\n",
    "Under ZOH, the state evolves on $[t_k, t_{k+1}]$ as $x(\\tau) = e^{A\\tau} x_k + \\Gamma(\\tau) u_k$ where $\\Gamma(\\tau) = \\int_0^\\tau e^{As} B\\, ds$.\n",
    "The exact integrated cost over interval $k$ is:\n",
    "$$\\ell_k = \\int_0^{\\Delta t_k} \\bigl(x(\\tau)^T Q\\, x(\\tau) + u_k^T R\\, u_k\\bigr)\\, d\\tau = z_k^T \\bar{W}_k z_k$$\n",
    "\n",
    "where $z_k = \\begin{bmatrix} x_k \\\\ u_k \\end{bmatrix}$ and $\\bar{W}_k = \\begin{bmatrix} Q_{d,k} & M_{d,k} \\\\ M_{d,k}^T & R_{d,k} \\end{bmatrix} \\succeq 0$ with:\n",
    "- $Q_{d,k} = \\int_0^{\\Delta t_k} e^{A^T\\tau} Q\\, e^{A\\tau}\\, d\\tau$ (observability-like Gramian)\n",
    "- $M_{d,k} = \\int_0^{\\Delta t_k} e^{A^T\\tau} Q\\, \\Gamma(\\tau)\\, d\\tau$ (cross term)\n",
    "- $R_{d,k} = \\int_0^{\\Delta t_k} \\Gamma(\\tau)^T Q\\, \\Gamma(\\tau)\\, d\\tau + \\Delta t_k R$\n",
    "\n",
    "### Van Loan Block Matrix Exponential\n",
    "\n",
    "Define $\\hat{A} = \\begin{bmatrix} A & B \\\\ 0 & 0 \\end{bmatrix}$ and $\\hat{Q} = \\begin{bmatrix} Q & 0 \\\\ 0 & 0 \\end{bmatrix}$. The cost-related integrals form the observability Gramian $W^Q = \\int_0^{\\Delta t} e^{\\hat{A}^T \\tau} \\hat{Q}\\, e^{\\hat{A}\\tau}\\, d\\tau$, extracted via:\n",
    "$$\\exp\\left(\\begin{bmatrix} -\\hat{A}^T & \\hat{Q} \\\\ 0 & \\hat{A} \\end{bmatrix} \\Delta t\\right) = \\begin{bmatrix} \\star & e^{-\\hat{A}^T \\Delta t}\\, W^Q \\\\ 0 & e^{\\hat{A}\\Delta t} \\end{bmatrix}$$\n",
    "\n",
    "From the bottom-right block we also read $A_d$ and $B_d$. The full cost matrix is $\\bar{W} = W^Q + \\text{diag}(0, \\Delta t\\, R)$.\n",
    "\n",
    "### Why This Stays a QP and Parameter-Affine\n",
    "\n",
    "The matrices $(\\bar{W}_k, A_{d,k}, B_{d,k})$ depend nonlinearly on $\\Delta t_k$ through `torch.matrix_exp`, but they are computed **outside** the QP. Inside the QP, they are fixed parameters:\n",
    "$$\\min_{x,u}\\;\\sum_k \\|L_k^T z_k\\|^2 \\quad \\text{s.t. } x_{k+1} = A_{d,k} x_k + B_{d,k} u_k,\\; |u_k| \\le u_{\\max}$$\n",
    "\n",
    "where $L_k = \\text{chol}(\\bar{W}_k)$ is a parameter matrix. The expression $L_k^T z_k$ is **affine in both parameters and variables**, so `cp.sum_squares(...)` is DPP-compliant in CVXPY. All nonlinearity in $\\Delta t$ resides in the parameter computation, preserving the QP structure.\n",
    "\n",
    "**Note on terminal cost:** No terminal cost $x_N^T P x_N$ is included, consistent with the other implementations in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient verification for exact ZOH cost pipeline\n",
    "def test_gradient_flow_zoh3(n_test=20):\n",
    "    \"\"\"Verify gradients flow through the full pipeline:\n",
    "    theta -> dt -> (Ad,Bd,W) via matrix_exp -> QP via CvxpyLayer -> loss.\"\"\"\n",
    "    A_t = torch.tensor(A, dtype=torch.float32)\n",
    "    B_t = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_t = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_t = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_t = torch.tensor(s0, dtype=torch.float32)\n",
    "\n",
    "    # Build a small CvxpyLayer for n_test\n",
    "    _, layer_t, _, _, _, _, _, _ = create_exact_zoh_cost_clqr(\n",
    "        n_test, s0, n_s, n_u, u_max,\n",
    "    )\n",
    "\n",
    "    theta_test = torch.nn.Parameter(torch.ones(n_test, 1))\n",
    "    dts_t = theta_2_dt(theta_test, T, n_test)\n",
    "\n",
    "    Ad_l, Bd_l, Lx_l, Lu_l, W_l = [], [], [], [], []\n",
    "    for k in range(n_test):\n",
    "        Ad_k, Bd_k, W_k = zoh_cost_matrices(dts_t[k], A_t, B_t, Q_t, R_t)\n",
    "        Ad_l.append(Ad_k)\n",
    "        Bd_l.append(Bd_k)\n",
    "        W_l.append(W_k)\n",
    "        L_k = torch.linalg.cholesky(W_k)\n",
    "        LT_k = L_k.T\n",
    "        Lx_l.append(LT_k[:, :n_s])\n",
    "        Lu_l.append(LT_k[:, n_s:])\n",
    "\n",
    "    sol_t = layer_t(*Ad_l, *Bd_l, *Lx_l, *Lu_l)\n",
    "\n",
    "    loss = torch.tensor(0.0, dtype=torch.float32)\n",
    "    for k in range(n_test):\n",
    "        s_k = s0_t if k == 0 else sol_t[k - 1]\n",
    "        u_k = sol_t[n_test + k]\n",
    "        z_k = torch.cat([s_k, u_k])\n",
    "        loss = loss + z_k @ W_l[k] @ z_k\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"Exact ZOH Integrated Cost - Gradient Check:\")\n",
    "    print(f\"  n_test = {n_test}\")\n",
    "    print(f\"  Loss value: {loss.item():.6f}\")\n",
    "    print(f\"  theta.grad exists: {theta_test.grad is not None}\")\n",
    "    if theta_test.grad is not None:\n",
    "        print(f\"  theta.grad norm: {theta_test.grad.norm().item():.6f}\")\n",
    "        print(f\"  theta.grad min/max: [{theta_test.grad.min().item():.6f}, \"\n",
    "              f\"{theta_test.grad.max().item():.6f}]\")\n",
    "        print(f\"  All finite: {torch.all(torch.isfinite(theta_test.grad)).item()}\")\n",
    "\n",
    "    # Finite-difference check on 5 random components\n",
    "    eps = 1e-4\n",
    "    n_fd = 5\n",
    "    indices = torch.randperm(n_test)[:n_fd]\n",
    "    grad_ad = theta_test.grad.flatten()[indices].detach().clone()\n",
    "    grad_fd = torch.zeros(n_fd)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        for sign, storage in [(1, 'plus'), (-1, 'minus')]:\n",
    "            theta_pert = theta_test.detach().clone()\n",
    "            theta_pert.flatten()[idx] += sign * eps\n",
    "            dts_p = theta_2_dt(theta_pert, T, n_test)\n",
    "\n",
    "            Ad_p, Bd_p, Lx_p, Lu_p, W_p = [], [], [], [], []\n",
    "            for k in range(n_test):\n",
    "                Ad_k, Bd_k, W_k = zoh_cost_matrices(dts_p[k], A_t, B_t, Q_t, R_t)\n",
    "                Ad_p.append(Ad_k)\n",
    "                Bd_p.append(Bd_k)\n",
    "                W_p.append(W_k)\n",
    "                L_k = torch.linalg.cholesky(W_k)\n",
    "                LT_k = L_k.T\n",
    "                Lx_p.append(LT_k[:, :n_s])\n",
    "                Lu_p.append(LT_k[:, n_s:])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sol_p = layer_t(*Ad_p, *Bd_p, *Lx_p, *Lu_p)\n",
    "            loss_p = 0.0\n",
    "            for k in range(n_test):\n",
    "                s_k = s0_t if k == 0 else sol_p[k - 1]\n",
    "                u_k = sol_p[n_test + k]\n",
    "                z_k = torch.cat([s_k, u_k])\n",
    "                loss_p += float(z_k @ W_p[k] @ z_k)\n",
    "            if sign == 1:\n",
    "                loss_plus = loss_p\n",
    "            else:\n",
    "                loss_minus = loss_p\n",
    "        grad_fd[i] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "    print(f\"\\n  Finite-difference check ({n_fd} components, eps={eps}):\")\n",
    "    print(f\"  {'Idx':>5s}  {'Autodiff':>12s}  {'Fin.Diff.':>12s}  {'Rel.Err':>12s}\")\n",
    "    for i in range(n_fd):\n",
    "        rel_err = abs(grad_ad[i] - grad_fd[i]) / (abs(grad_ad[i]) + 1e-10)\n",
    "        print(f\"  {indices[i].item():5d}  {grad_ad[i].item():12.6f}  \"\n",
    "              f\"{grad_fd[i].item():12.6f}  {rel_err.item():12.6e}\")\n",
    "\n",
    "test_gradient_flow_zoh3(n_test=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54239fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh_3 = [\"exact_zoh_integrated\"]\n",
    "\n",
    "_rm = get_method_run_mode(run_mode, \"zoh3\", run_overrides)\n",
    "if _rm != RunMode.DISPLAY:\n",
    "    n_epochs = get_n_epochs(_rm, \"zoh3\", n_epochs_override)\n",
    "    history_zoh_3 = []\n",
    "    sol_zoh_3 = {}\n",
    "\n",
    "    A_torch = torch.tensor(A, dtype=torch.float32)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_torch = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_torch = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_torch = torch.tensor(s0, dtype=torch.float32)\n",
    "\n",
    "    method = \"exact_zoh_integrated\"\n",
    "\n",
    "    theta = torch.nn.Parameter(torch.ones(n_zoh3, 1))\n",
    "    optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "    _, layer_3, _, _, _, _, _, _ = create_exact_zoh_cost_clqr(\n",
    "        n_zoh3, s0, n_s, n_u, u_max,\n",
    "    )\n",
    "\n",
    "    with tqdm(total=n_epochs) as pbar:\n",
    "        for epoch in range(n_epochs):\n",
    "            pbar.update(1)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            dts_torch = theta_2_dt(theta, T, n_zoh3)\n",
    "\n",
    "            Ad_list, Bd_list, Lx_list, Lu_list, W_list = [], [], [], [], []\n",
    "            for k in range(n_zoh3):\n",
    "                Ad_k, Bd_k, W_k = zoh_cost_matrices(\n",
    "                    dts_torch[k], A_torch, B_torch, Q_torch, R_torch,\n",
    "                )\n",
    "                Ad_list.append(Ad_k)\n",
    "                Bd_list.append(Bd_k)\n",
    "                W_list.append(W_k)\n",
    "\n",
    "                L_k = torch.linalg.cholesky(W_k)\n",
    "                LT_k = L_k.T\n",
    "                Lx_list.append(LT_k[:, :n_s])\n",
    "                Lu_list.append(LT_k[:, n_s:])\n",
    "\n",
    "            sol_zoh_3[method] = layer_3(*Ad_list, *Bd_list, *Lx_list, *Lu_list)\n",
    "\n",
    "            loss = torch.tensor(0.0, dtype=torch.float32)\n",
    "            for k in range(n_zoh3):\n",
    "                s_k = s0_torch if k == 0 else sol_zoh_3[method][k - 1]\n",
    "                u_k = sol_zoh_3[method][n_zoh3 + k]\n",
    "                z_k = torch.cat([s_k, u_k])\n",
    "                loss = loss + z_k @ W_list[k] @ z_k\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            history_zoh_3.append({\n",
    "                \"method\": method,\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": float(loss.item()),\n",
    "                \"dts\": dts_torch.detach().cpu().numpy(),\n",
    "            })\n",
    "\n",
    "    _pkl_sol = pickle_name(\"sol_zoh_3\", n_zoh3, n)\n",
    "    _pkl_hist = pickle_name(\"history_zoh_3\", n_zoh3, n)\n",
    "    save_pickle(out_dir, _pkl_sol, sol_zoh_3)\n",
    "    save_pickle(out_dir, _pkl_hist, history_zoh_3)\n",
    "else:\n",
    "    _pkl_sol = pickle_name(\"sol_zoh_3\", n_zoh3, n)\n",
    "    _pkl_hist = pickle_name(\"history_zoh_3\", n_zoh3, n)\n",
    "    sol_zoh_3 = load_pickle(out_dir, _pkl_sol)\n",
    "    history_zoh_3 = load_pickle(out_dir, _pkl_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a70a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in loss_methods_zoh_3:\n",
    "    hist_m = [h for h in history_zoh_3 if h['method'] == method]\n",
    "    plot_training_res(sol_zoh_3[method], hist_m, n_zoh3, sol_method=2)\n",
    "    save_training_res(\n",
    "        \"out\", method + \"_zoh3\",\n",
    "        sol_zoh_3[method], hist_m, n_zoh3, sol_method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8487dc",
   "metadata": {},
   "source": [
    "## Sampling Density and Trajectory Change Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d66d8",
   "metadata": {},
   "source": [
    "### Collect All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c275849",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_solutions = {}\n",
    "\n",
    "method_configs = [\n",
    "    (\"Aux\", loss_methods, sol_aux, history_aux, 1, n),\n",
    "    (\"Rep\", loss_methods_2, sol_rep, history_rep, 2, n),\n",
    "    (\"HS\", loss_methods_hs, sol_hs, history_hs, 2, n),\n",
    "    (\"ZOH\", loss_methods_zoh, sol_zoh, history_zoh, 2, n),\n",
    "    (\"ZOH2\", loss_methods_zoh_2, sol_zoh_2, history_zoh_2, 2, n),\n",
    "    (\"ZOH3\", loss_methods_zoh_3, sol_zoh_3, history_zoh_3, 2, n_zoh3),\n",
    "]\n",
    "\n",
    "for prefix, methods, sol_dict, history_list, sol_method, n_method in method_configs:\n",
    "    try:\n",
    "        for method in methods:\n",
    "            label = f\"{prefix} (n={n_method}): {method}\" if n_method != n else f\"{prefix}: {method}\"\n",
    "            method_solutions[label] = {\n",
    "                'sol': sol_dict[method],\n",
    "                'history': [h for h in history_list if h['method'] == method],\n",
    "                'sol_method': sol_method,\n",
    "                'n': n_method,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {prefix} methods: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(method_solutions)} methods: {list(method_solutions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ac34c",
   "metadata": {},
   "source": [
    "### Sampling Density vs Trajectory Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_methods = len(method_solutions)\n",
    "n_rows = int(np.ceil(n_methods / 2))\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(10, 2.5 * n_rows), squeeze=False)\n",
    "\n",
    "for i, (key, ms) in enumerate(method_solutions.items()):\n",
    "    n_m = ms['n']\n",
    "    data = extract_trajectory_data(ms, n_m)\n",
    "    metrics = compute_trajectory_metrics(data, n_m, T)\n",
    "    plot_density_and_changes(data, metrics, key, colors, axes=axs[i // 2, i % 2])\n",
    "\n",
    "for j in range(i + 1, n_rows * 2):\n",
    "    fig.delaxes(axs[j // 2, j % 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae984e",
   "metadata": {},
   "source": [
    "### Cross-Correlation (Time-Lagged)\n",
    "\n",
    "Cross correlation of the Sampling Density (SD) with:\n",
    "- $\\| \\Delta u \\|$\n",
    "- $\\| \\Delta s \\|_2$\n",
    "- $\\| u \\|$\n",
    "- $\\| s \\|_2$\n",
    "\n",
    "On the y-axis, the cross-correlation factor.\n",
    "\n",
    "The box indicates the maximum CC and the time lag at which it happens.\n",
    "\n",
    "The dotted lines at $\\pm 1.96 / \\sqrt{n}$ â‰ˆ Â±0.155 (for n=160) are the 95% CI. Outside bounds -> statistically significant correlation.\n",
    "\n",
    "- **lag = 0**: Instantaneous correlation\n",
    "- **lag > 0**: Does high sampling density *precede* large changes?\n",
    "- **lag < 0**: Does high sampling density *follow* large changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ms in method_solutions.items():\n",
    "    n_m = ms['n']\n",
    "    data = extract_trajectory_data(ms, n_m)\n",
    "    metrics = compute_trajectory_metrics(data, n_m, T)\n",
    "    plot_cross_correlations(data, metrics, key, colors, max_lag=30)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
