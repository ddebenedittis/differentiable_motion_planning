{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dimp.utils import init_matplotlib, get_colors\n",
    "\n",
    "\n",
    "init_matplotlib()\n",
    "\n",
    "colors = get_colors()\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Change rerun to True if you want to re-run the experiments and save the results.\n",
    "\n",
    "rerun = False\n",
    "\n",
    "out_dir = \"data/pann_clqr_dt\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63114ab5",
   "metadata": {},
   "source": [
    "## Example Pannocchia ðŸŒ½\n",
    "\n",
    "Contnuous LTI system with dynamics\n",
    "$$\n",
    "\\dot{s} = A s + B u\n",
    "$$\n",
    "\n",
    "3 states and 1 input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc57f6",
   "metadata": {},
   "source": [
    "### Define the problem matrices, the initial state, the time horizon, and the LQR matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [-0.1, 0, 0],\n",
    "    [0, -2, -6.25],\n",
    "    [0, 4, 0]\n",
    "])\n",
    "\n",
    "B = np.array([[0.25], [2.0], [0.0]])\n",
    "\n",
    "s0 = np.array([1.344, -4.585, 5.674])   # initial state\n",
    "\n",
    "T = 10.0        # time window\n",
    "\n",
    "N = 1000        # max number of timesteps of the OCP\n",
    "\n",
    "dt = T / N\n",
    "\n",
    "Q = 1.0 * np.eye(3)\n",
    "R = 0.1 * np.eye(1)\n",
    "\n",
    "u_max = 1.0     # max control input\n",
    "\n",
    "n_s = 3         # number of states\n",
    "n_u = 1         # number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207e76c",
   "metadata": {},
   "source": [
    "### Create the Cxvpy Variables and the Optimization Problem\n",
    "\n",
    "Classic constrained LQR problem.\n",
    "Discretized with first-order Euler.\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left( s_k^T Q s_k + u_k^T R u_k \\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = s_k + \\Delta t (A s_k + B u_k), \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [s0] + [cp.Variable(3, name=f\"s_{i}\") for i in range(N)]\n",
    "u = [cp.Variable(1, name=f\"u_{i}\") for i in range(N)]\n",
    "\n",
    "def create_pann_clqr(n: int = N, dt: float = dt):\n",
    "    objective = cp.Minimize(\n",
    "        cp.sum([cp.quad_form(s[i+1], Q) for i in range(n)]) * dt +\n",
    "        cp.sum([cp.quad_form(u[i], R) for i in range(n)]) * dt\n",
    "    )\n",
    "    \n",
    "    dynamics_constraints = [\n",
    "        s[i+1] == s[i] + (A @ s[i] + B @ u[i]) * dt for i in range(n)\n",
    "    ]\n",
    "    \n",
    "    input_limits = [\n",
    "        cp.abs(u[i]) <= u_max for i in range(n)\n",
    "    ]\n",
    "\n",
    "    constraints = dynamics_constraints + input_limits\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "pann_clqr = create_pann_clqr()\n",
    "assert pann_clqr.is_dpp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59beda2",
   "metadata": {},
   "source": [
    "### Study how the solution changes with different number of samples.\n",
    "\n",
    "With $n=80$ samples the system is unstable.\n",
    "From $n=160$ timesteps the OCP stabilizes the system.\n",
    "However, increasing the number of timesteps further improves the solution (see the optimal cost value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of timesteps of the trajectory optimization problem.\n",
    "interval = 80\n",
    "\n",
    "fig, axs = plt.subplots(12, 2, figsize=(6.4, 12.8), constrained_layout=True)\n",
    "\n",
    "i = 0\n",
    "for n in range(interval, N + 1, interval):\n",
    "    dt = T / n\n",
    "    pann_clqr = create_pann_clqr(n=n, dt=dt)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pann_clqr.solve()\n",
    "    solve_time = time.time() - start_time\n",
    "    \n",
    "    print(\n",
    "        f\"n = {n}\\n\"\n",
    "        f\"Optimal cost: {pann_clqr.objective.value:.4f}, \"\n",
    "        f\"solve time meas: {solve_time:.4f} s, \"\n",
    "        f\"solve time: {pann_clqr.solver_stats.solve_time:.4f} s\\n\"\n",
    "    )\n",
    "    \n",
    "    # if n > 6*interval:\n",
    "    #     continue\n",
    "\n",
    "    times = np.arange(n) * dt\n",
    "    s_vec = np.array([s.value for s in s[1:n+1]])\n",
    "    u_vec = np.array([u.value for u in u[:n]])\n",
    "\n",
    "    axs[2*(i//2), i%2].plot(times, s_vec, label=['x', 'y', 'z'])\n",
    "    axs[2*(i//2), i%2].set(\n",
    "        xlabel='Time',\n",
    "        ylabel='State',\n",
    "        title=fr\"$n={n}$\",\n",
    "    )\n",
    "\n",
    "    axs[2*(i//2)+1, i%2].plot(times, u_vec)\n",
    "    axs[2*(i//2)+1, i%2].set(\n",
    "        xlabel='Time',\n",
    "        ylabel='Input',\n",
    "    )\n",
    "\n",
    "    # axs[i//2, (i+1)%2].legend()\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b6954",
   "metadata": {},
   "source": [
    "## DQP with Auxiliary Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e1ce4",
   "metadata": {},
   "source": [
    "### Create the Parametrized CLQR Problem\n",
    "\n",
    "Discretize then linearize the (nonlinear) dynamics.\n",
    "$$\n",
    "s_{k+1} = \\bar{s}_k + \\bar{\\delta}_k (A \\bar{s}_k + B u_k) + (I + \\bar{\\delta}_k A) \\tilde{s}_k + \\bar{\\delta}_k B u_k + (A \\bar{s}_k + B u_k) \\tilde{\\delta}_k,\n",
    "$$\n",
    "where $\\tilde{\\square} = \\square - \\bar{\\square}$ and $\\delta_k$ is an auxiliary variable that represents the time step lengths.\n",
    "\n",
    "The optimization vector of the QP is $\\begin{bmatrix} \\tilde{s}_{k=1,\\dots,N} & \\tilde{u}_{k=0,\\dots,N-1} & \\tilde{\\delta}_{k=0,\\dots,N} -1\\end{bmatrix}$.\n",
    "\n",
    "The QP is parametrized by the parameter vector $\\bar{\\delta} = \\begin{bmatrix} \\bar{\\delta}_1 & \\dots & \\bar{\\delta}_N \\end{bmatrix}^T$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k, \\delta_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left(\\bar{\\delta}_k \\left( s_k^T Q s_k + u_k^T R u_k \\right) + w_\\delta \\tilde{\\delta}_k^2 \\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = \\bar{s}_k + \\bar{\\delta}_k (A \\bar{s}_k + B \\bar{u}_k) + (I + \\bar{\\delta}_k A) \\tilde{s}_k + \\bar{\\delta}_k B \\tilde{u}_k + (A \\bar{s}_k + B \\bar{u}_k) \\tilde{\\delta}_k, \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03087086",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "deltas = [cp.Variable(1, name=f'deltas_{i}') for i in range(n)]\n",
    "dts = [cp.Parameter(1, nonneg=True, name=f'dts_{i}') for i in range(n)]\n",
    "\n",
    "def create_pann_param_clqr(s_bar, u_bar, n: int = N):\n",
    "    objective = cp.Minimize(\n",
    "        cp.sum([cp.quad_form(s[i+1], Q) * dts[i] for i in range(n)]) +\n",
    "        cp.sum([cp.quad_form(u[i], R) * dts[i] for i in range(n)]) +\n",
    "        10**3 * cp.sum([cp.square(deltas[i] - dts[i]) for i in range(n)])\n",
    "    )\n",
    "    \n",
    "    dynamics_constraints = [\n",
    "        s[i+1] == s_bar[i] \\\n",
    "            + dts[i] * (A @ s_bar[i] + B @ u_bar[i]) \\\n",
    "            + (np.eye(n_s) + dts[i] * A) @ (s[i] - s_bar[i]) \\\n",
    "            + dts[i] * B @ (u[i] - u_bar[i]) \\\n",
    "            + (deltas[i] - dts[i]) * (A @ s_bar[i] + B @ u_bar[i]) \\\n",
    "            for i in range(n)\n",
    "    ]\n",
    "    \n",
    "    input_limits = [\n",
    "        cp.abs(u[i]) <= u_max for i in range(n)\n",
    "    ]\n",
    "    \n",
    "    timestep_constraints = [\n",
    "        deltas[i] >= 0 for i in range(n)\n",
    "    ] + [\n",
    "        cp.sum(deltas[0:n]) == T\n",
    "    ]\n",
    "\n",
    "    constraints = dynamics_constraints + input_limits + timestep_constraints\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "s_bar = [np.zeros(n_s) for _ in range(n)]\n",
    "u_bar = [np.zeros(n_u) for _ in range(n)]\n",
    "\n",
    "pann_param_clqr = create_pann_param_clqr(s_bar, u_bar, n)\n",
    "assert pann_param_clqr.is_dpp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f61a6e",
   "metadata": {},
   "source": [
    "### Task Loss\n",
    "\n",
    "Unscaled\n",
    "$$\n",
    "\\mathcal{L}_1 = \\sum_{k=0}^{N} \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)\n",
    "$$\n",
    "\n",
    "Time scaled\n",
    "$$\n",
    "\\mathcal{L}_2 = \\sum_{k=0}^{N} \\delta_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)\n",
    "$$\n",
    "\n",
    "Time bar scaled\n",
    "$$\n",
    "\\mathcal{L}_3 = \\sum_{k=0}^{N} \\bar{\\delta}_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da51f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_methods = [\"unscaled\", \"time scaled\", \"time bar scaled\"]\n",
    "loss_methods = [\"time scaled\", \"time bar scaled\"]\n",
    "\n",
    "def task_loss(sol, param, method=\"unscaled\"):\n",
    "    states = [sol[i] for i in range(n)]\n",
    "    inputs = [sol[n+i] for i in range(n)]\n",
    "    deltas = [sol[2*n+i] for i in range(n)]\n",
    "    deltas_bar = np.concatenate([d_bar.detach().numpy() for d_bar in param])\n",
    "    \n",
    "    Q_th = torch.tensor(Q, dtype=torch.float32, device=states[0].device)\n",
    "    R_th = torch.tensor(R, dtype=torch.float32, device=states[0].device)\n",
    "    \n",
    "    if method == 'unscaled':\n",
    "        return sum([\n",
    "            si.t() @ Q_th @ si for si in states\n",
    "        ]) + sum([\n",
    "            ui.t() @ R_th @ ui for ui in inputs\n",
    "        ])\n",
    "    if method == 'time scaled':\n",
    "        return sum([\n",
    "            deltas[i] * states[i].t() @ Q_th @ states[i] for i in range(n)\n",
    "        ]) + sum([\n",
    "            deltas[i] * inputs[i].t() @ R_th @ inputs[i] for i in range(n)\n",
    "        ])\n",
    "    if method == 'time bar scaled':\n",
    "        return sum([\n",
    "            deltas_bar[i] * states[i].t() @ Q_th @ states[i] for i in range(n)\n",
    "        ]) + sum([\n",
    "            deltas_bar[i] * inputs[i].t() @ R_th @ inputs[i] for i in range(n)\n",
    "        ])\n",
    "    \n",
    "    raise ValueError(f\"Unknown method {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ec3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timegrid(deltas, x=None, ax=None, ylabel=None, title=None):\n",
    "    times = np.cumsum(deltas.tolist())\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for t in times:\n",
    "        ax.axvline(t, color='gray', linestyle='--', alpha=0.25)\n",
    "    \n",
    "    if x is not None:\n",
    "        ax.plot(times, x)\n",
    "        \n",
    "    ax.set_xlabel(\"Time\")\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c81d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colored(deltas, x, ax=None):\n",
    "    times = np.cumsum(deltas)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    norm = Normalize(vmin=np.min(deltas), vmax=np.max(deltas))\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    for i in range(len(x)-1):\n",
    "        # horizontal hold\n",
    "        ax.hlines(x[i], times[i], times[i+1], \n",
    "                  colors=cmap(norm(deltas[i+1])), linewidth=2)\n",
    "        # vertical jump\n",
    "        ax.vlines(times[i+1], x[i], x[i+1], \n",
    "                  colors=cmap(norm(deltas[i+1])), linewidth=1)\n",
    "        \n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Input\")\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"deltas\")\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fee1b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_res(sol, history, method: int):\n",
    "    s_arr = np.array([s.detach().numpy().tolist() for s in sol[0:n]])\n",
    "    u_arr = np.array([u.detach().numpy().tolist() for u in sol[n:2*n]])\n",
    "    if method == 1:\n",
    "        d_arr = np.concatenate(np.array([d.detach().numpy().tolist() for d in sol[2*n:3*n]]))\n",
    "    elif method == 2:\n",
    "        d_arr = history[-1]['dts']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(9.6, 9.6))\n",
    "    ax[0, 0].plot([h['loss'] for h in history])\n",
    "\n",
    "    ax[0, 0].set_xlabel(\"Epoch\")\n",
    "    ax[0, 0].set_ylabel(\"Loss\")\n",
    "    ax[0, 0].set_title(\"Loss Evolution\")\n",
    "    \n",
    "    plot_colored(d_arr, u_arr, ax[1, 0])\n",
    "    \n",
    "    plot_timegrid(\n",
    "        d_arr, s_arr, ax[0, 1],\n",
    "        ylabel=\"State\",\n",
    "        title=\"State Evolution\"\n",
    "    )\n",
    "\n",
    "    plot_timegrid(\n",
    "        d_arr, u_arr, ax[1, 1],\n",
    "        ylabel=\"Input\",\n",
    "        title=\"Input Evolution\"\n",
    "    )\n",
    "    \n",
    "    ax[0, 2].plot(np.cumsum(d_arr), d_arr)\n",
    "    ax[0, 2].set_xlabel(\"Time\")\n",
    "    ax[0, 2].set_ylabel(\"Timestep duration\")\n",
    "    ax[0, 2].set_title(\"Timesteps Evolution\")\n",
    "    \n",
    "    fig.delaxes(ax[1, 2])\n",
    "\n",
    "    fig.set_constrained_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_res(exp_name: str, sol, history, method: int):\n",
    "    out_dir = f\"out/{exp_name}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    s_arr = np.array([s.detach().numpy().tolist() for s in sol[0:n]])\n",
    "    u_arr = np.array([u.detach().numpy().tolist() for u in sol[n:2*n]])\n",
    "    if method == 1:\n",
    "        d_arr = np.concatenate(np.array([d.detach().numpy().tolist() for d in sol[2*n:3*n]]))\n",
    "    elif method == 2:\n",
    "        d_arr = history[-1]['dts']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3.2, 3.2))\n",
    "    plot_colored(d_arr, u_arr, ax)\n",
    "    fig.savefig(\n",
    "        f\"{out_dir}/input.pdf\", bbox_inches='tight',\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3.2, 3.2))\n",
    "    ax.plot(np.cumsum(d_arr), d_arr)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Timestep duration\")\n",
    "    fig.savefig(\n",
    "        f\"{out_dir}/timesteps.pdf\", bbox_inches='tight',\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3.2, 3.2))\n",
    "    ax.plot([h['loss'] for h in history])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    fig.savefig(\n",
    "        f\"{out_dir}/loss.pdf\", bbox_inches='tight',\n",
    "    )\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun:\n",
    "    n_epochs = 100\n",
    "\n",
    "    history_aux  = []\n",
    "    sol_aux = {}\n",
    "\n",
    "    for method in loss_methods:\n",
    "        print(f\"Method: {method}\")\n",
    "        \n",
    "        n = 160\n",
    "\n",
    "        dts_torch = [torch.nn.Parameter(torch.ones(1) * dt) for _ in range(n)]\n",
    "\n",
    "        optim = torch.optim.Adam(dts_torch, lr=5e-4)\n",
    "\n",
    "        # =========================================================================== #\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dt = T / n\n",
    "            for d in dts_torch:\n",
    "                d.copy_(torch.ones(1) * dt)\n",
    "\n",
    "        s_bar = [s0 + (np.zeros(n_s) - s0) * i/n for i in range(n)]\n",
    "        u_bar = [np.zeros(n_u) for _ in range(n)]\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "\n",
    "                pann_param_clqr = create_pann_param_clqr(s_bar, u_bar, n)\n",
    "                cvxpylayer = CvxpyLayer(\n",
    "                    pann_param_clqr,\n",
    "                    parameters=dts[:n],\n",
    "                    variables=s[1:n+1] + u[:n] + deltas[:n],\n",
    "                )\n",
    "                sol_aux[method] = cvxpylayer(*dts_torch)\n",
    "                            \n",
    "                s_bar = [sol_aux[method][i].detach().numpy() for i in range(n)]\n",
    "                u_bar = [sol_aux[method][n+i].detach().numpy() for i in range(n)]\n",
    "\n",
    "                loss = task_loss(sol_aux[method], dts_torch, method=method)\n",
    "                loss.backward()\n",
    "\n",
    "                optim.step()\n",
    "                with torch.no_grad():\n",
    "                    for d in dts_torch:\n",
    "                        d.clamp_(min=1e-6, max=0.07)\n",
    "                        d *= T / sum(dts_torch)\n",
    "\n",
    "                history_aux.append({\n",
    "                    'method': method,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': [d.detach().numpy() for d in dts_torch]\n",
    "                })\n",
    "\n",
    "    with open(f\"{out_dir}/sol_aux.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sol_aux, f)\n",
    "    with open(f\"{out_dir}/history_aux.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history_aux, f)\n",
    "else:\n",
    "    with open(f\"{out_dir}/sol_aux.pkl\", \"rb\") as f:\n",
    "        sol_aux = pickle.load(f)\n",
    "    with open(f\"{out_dir}/history_aux.pkl\", \"rb\") as f:\n",
    "        history_aux = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "for method in loss_methods:\n",
    "    plot_training_res(sol_aux[method], [h for h in history_aux if h['method'] == method], method=1)\n",
    "    save_training_res(\n",
    "        method.replace(\" \", \"_\"),\n",
    "        sol_aux[method],\n",
    "        [h for h in history_aux if h['method'] == method],\n",
    "        method=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ad0c2",
   "metadata": {},
   "source": [
    "## DQP with Reparametrized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_2_dt(theta):\n",
    "    eps = 1e-3\n",
    "    \n",
    "    w = torch.softmax(theta.flatten(), dim=0)\n",
    "    return eps + (T - n*eps) * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76272a",
   "metadata": {},
   "source": [
    "### DQP With Reparametrized Timesteps\n",
    "\n",
    "Reparametrize time steps on the simplex:\n",
    "$$\n",
    "\\Delta t_k = \\epsilon + (T - n \\epsilon) \\frac{e^{\\theta_k}}{\\sum_j e^{\\theta_j}}\n",
    "$$\n",
    "This enforces both positivity and the total time constraint $\\sum_k \\Delta t_k = T$ without discontinuous updates.\n",
    "\n",
    "The optimization vector of the QP is $\\begin{bmatrix} \\tilde{s}_{k=1,\\dots,N} & \\tilde{u}_{k=0,\\dots,N-1}\\end{bmatrix}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\min_{\\substack{s_{k+1}, u_k \\\\ k=0, \\dots, N}} \\quad & \\sum_{k=0}^{N} \\left(\\Delta t_k \\left( s_k^T Q s_k + u_k^T R u_k \\right)\\right), \\\\\n",
    "& \\text{s.t.} \\quad & s_{k+1} = \\bar{s}_k + \\Delta t_k (A \\bar{s}_k + B \\bar{u}_k) + (I + \\Delta t_k A) \\tilde{s}_k + \\Delta t_k B \\tilde{u}_k, \\\\\n",
    "& & s_0 = s_{\\text{init}}, \\\\\n",
    "& & u_k \\in U.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The OCP uses $\\Delta t_k$ as a parameter.\n",
    "(The optimizer uses $\\theta_k$ as optimization parameters.)\n",
    "Gradients do flow through the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "\n",
    "def create_pann_param_clqr_2(n: int = N):\n",
    "    dts = cp.Parameter(n, nonneg=True, name='dts')\n",
    "    \n",
    "    objective = cp.Minimize(\n",
    "        cp.sum([cp.quad_form(s[i+1], Q) * dts[i] for i in range(n)]) +\n",
    "        cp.sum([cp.quad_form(u[i], R) * dts[i] for i in range(n)])\n",
    "    )\n",
    "    \n",
    "    dynamics_constraints = [\n",
    "        s[i+1] == s[i] \\\n",
    "            + dts[i] * (A @ s[i] + B @ u[i]) \\\n",
    "            for i in range(n)\n",
    "    ]\n",
    "    \n",
    "    input_limits = [\n",
    "        cp.abs(u[i]) <= u_max for i in range(n)\n",
    "    ]\n",
    "\n",
    "    constraints = dynamics_constraints + input_limits\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    \n",
    "    return problem, dts\n",
    "\n",
    "pann_param_clqr_2, _ = create_pann_param_clqr_2(n)\n",
    "assert pann_param_clqr_2.is_dpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef024b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_2 = [\"unscaled\", \"time scaled\"]\n",
    "loss_methods_2 = [\"time scaled\"]\n",
    "\n",
    "def task_loss_2(sol, param, method=\"unscaled\"):\n",
    "    states = [sol[i] for i in range(n)]\n",
    "    inputs = [sol[n+i] for i in range(n)]\n",
    "    deltas = param\n",
    "    \n",
    "    Q_th = torch.tensor(Q, dtype=torch.float32, device=states[0].device)\n",
    "    R_th = torch.tensor(R, dtype=torch.float32, device=states[0].device)\n",
    "    \n",
    "    if method == 'unscaled':\n",
    "        return sum([\n",
    "            si.t() @ Q_th @ si for si in states\n",
    "        ]) + sum([\n",
    "            ui.t() @ R_th @ ui for ui in inputs\n",
    "        ])\n",
    "    if method == 'time scaled':\n",
    "        return sum([\n",
    "            deltas[i] * states[i].t() @ Q_th @ states[i] for i in range(n)\n",
    "        ]) + sum([\n",
    "            deltas[i] * inputs[i].t() @ R_th @ inputs[i] for i in range(n)\n",
    "        ])\n",
    "    \n",
    "    raise ValueError(f\"Unknown method {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ab1a3",
   "metadata": {},
   "source": [
    "### Task Loss\n",
    "\n",
    "Unscaled\n",
    "$$\n",
    "\\mathcal{L}_1 = \\sum_{k=0}^{N} \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)\n",
    "$$\n",
    "\n",
    "Time scaled\n",
    "$$\n",
    "\\mathcal{L}_2 = \\sum_{k=0}^{N} \\Delta t_k \\left(\\lVert s_k \\rVert^2_Q  + \\lVert u_k \\rVert^2_R \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06278de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun:\n",
    "    n_epochs = 200\n",
    "\n",
    "    history_rep  = []\n",
    "    sol_rep = {}\n",
    "\n",
    "    for method in loss_methods_2:\n",
    "        print(f\"Method: {method}\")\n",
    "        \n",
    "        n = 160\n",
    "\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        # =========================================================================== #\n",
    "\n",
    "        s_bar = [s0 + (np.zeros(n_s) - s0) * i/n for i in range(n)]\n",
    "        u_bar = [np.zeros(n_u) for _ in range(n)]\n",
    "        with tqdm(total=n_epochs) as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                pbar.update(1)\n",
    "                \n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                dts2_torch = theta_2_dt(theta)\n",
    "\n",
    "                pann_param_clqr, dts2 = create_pann_param_clqr_2(n)\n",
    "                cvxpylayer = CvxpyLayer(\n",
    "                    pann_param_clqr,\n",
    "                    parameters=[dts2],\n",
    "                    variables=s[1:n+1] + u[:n],\n",
    "                )\n",
    "                sol_rep[method] = cvxpylayer(dts2_torch)\n",
    "                \n",
    "                s_bar = [sol_rep[method][i].detach().numpy() for i in range(n)]\n",
    "                u_bar = [sol_rep[method][n+i].detach().numpy() for i in range(n)]\n",
    "\n",
    "                loss = task_loss_2(sol_rep[method], dts2_torch, method=method)\n",
    "                loss.backward()\n",
    "\n",
    "                optim.step()\n",
    "\n",
    "                history_rep.append({\n",
    "                    'method': method,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': dts2_torch.detach().numpy(),\n",
    "                })\n",
    "                \n",
    "    with open(f\"{out_dir}/history_rep.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history_rep, f)\n",
    "    with open(f\"{out_dir}/sol_rep.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sol_rep, f)\n",
    "else:\n",
    "    with open(f\"{out_dir}/history_rep.pkl\", \"rb\") as f:\n",
    "        history_rep = pickle.load(f)\n",
    "    with open(f\"{out_dir}/sol_rep.pkl\", \"rb\") as f:\n",
    "        sol_rep = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b019321",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "for method in loss_methods_2:\n",
    "    plot_training_res(sol_rep[method], [h for h in history_rep if h['method'] == method], method=2)\n",
    "    save_training_res(\n",
    "        method.replace(\" \", \"_\") + \"_rep\",\n",
    "        sol_rep[method],\n",
    "        [h for h in history_rep if h['method'] == method],\n",
    "        method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f2fd2",
   "metadata": {},
   "source": [
    "## Loss Hyper Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b402a35",
   "metadata": {},
   "source": [
    "### LHS - Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a11069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoh_discretize(dt, A, B):\n",
    "    \"\"\"\n",
    "    Compute exact ZOH discretization (Ad, Bd) via matrix exponential.\n",
    "    Fully differentiable through torch.matrix_exp.\n",
    "    \n",
    "    Args:\n",
    "        dt: Scalar timestep duration (torch tensor)\n",
    "        A: Continuous-time state matrix (n_s, n_s)\n",
    "        B: Continuous-time input matrix (n_s, n_u)\n",
    "    \n",
    "    Returns:\n",
    "        Ad: Discrete-time state matrix (n_s, n_s)\n",
    "        Bd: Discrete-time input matrix (n_s, n_u)\n",
    "    \"\"\"\n",
    "    n_s, n_u = A.shape[0], B.shape[1]\n",
    "    M = torch.zeros(n_s + n_u, n_s + n_u, device=dt.device, dtype=A.dtype)\n",
    "    M[:n_s, :n_s] = A * dt\n",
    "    M[:n_s, n_s:] = B * dt\n",
    "    E = torch.matrix_exp(M)\n",
    "    return E[:n_s, :n_s], E[:n_s, n_s:]\n",
    "\n",
    "\n",
    "def uniform_resampling_loss(\n",
    "    inputs_qp,      # list of n tensors from QP solution, each shape (n_u,)\n",
    "    dts_torch,      # shape (n,), timesteps summing to T\n",
    "    s0,             # initial state, shape (n_s,)\n",
    "    A, B,           # system matrices (torch tensors)\n",
    "    Q, R,           # cost matrices (torch tensors)\n",
    "    T,              # total horizon\n",
    "    n_res=1000,     # number of uniform grid points\n",
    "    use_exact=False # if True, use matrix_exp; else Euler\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate LQR cost on a dense uniform time grid.\n",
    "    \n",
    "    This loss function:\n",
    "    1. Creates a uniform grid of n_res points over [0, T]\n",
    "    2. Interpolates inputs using Zero-Order Hold from QP solution\n",
    "    3. Simulates state forward using Euler (or exact) integration\n",
    "    4. Computes Riemann sum approximation to continuous cost integral\n",
    "    \n",
    "    Gradients flow through inputs_qp -> QP solution -> dts_torch -> theta.\n",
    "    The interval indices are computed with detached cumsum (discrete, no grad needed).\n",
    "    \"\"\"\n",
    "    device = dts_torch.device\n",
    "    dtype = dts_torch.dtype\n",
    "    n = len(dts_torch)\n",
    "    \n",
    "    # Ensure tensors are on correct device\n",
    "    A = torch.as_tensor(A, dtype=dtype, device=device)\n",
    "    B = torch.as_tensor(B, dtype=dtype, device=device)\n",
    "    Q = torch.as_tensor(Q, dtype=dtype, device=device)\n",
    "    R = torch.as_tensor(R, dtype=dtype, device=device)\n",
    "    s0 = torch.as_tensor(s0, dtype=dtype, device=device)\n",
    "    \n",
    "    # Dense uniform time grid\n",
    "    dt_uniform = T / n_res\n",
    "    t_uniform = torch.linspace(0, T - dt_uniform, n_res, device=device, dtype=dtype)\n",
    "    \n",
    "    # Cumulative times (end of each interval)\n",
    "    t_cumsum = torch.cumsum(dts_torch, dim=0)\n",
    "    \n",
    "    # ZOH interpolation: find interval index for each uniform time point\n",
    "    # Detach for index computation (indices are discrete, no gradient needed)\n",
    "    indices = torch.searchsorted(t_cumsum.detach(), t_uniform, right=False)\n",
    "    indices = torch.clamp(indices, 0, n - 1)\n",
    "    \n",
    "    # Stack inputs and index into them (gradients flow through u_stack)\n",
    "    u_stack = torch.stack(inputs_qp, dim=0)  # (n, n_u)\n",
    "    u_interp = u_stack[indices]  # (n_res, n_u)\n",
    "    \n",
    "    # Forward simulation on uniform grid\n",
    "    s_list = []\n",
    "    s_current = s0.clone()\n",
    "    \n",
    "    if use_exact:\n",
    "        # Pre-compute discrete matrices for uniform timestep\n",
    "        dt_uniform_t = torch.tensor(dt_uniform, device=device, dtype=dtype)\n",
    "        Ad_uniform, Bd_uniform = zoh_discretize(dt_uniform_t, A, B)\n",
    "        for j in range(n_res):\n",
    "            s_list.append(s_current)\n",
    "            s_current = Ad_uniform @ s_current + Bd_uniform @ u_interp[j]\n",
    "    else:\n",
    "        # Euler integration\n",
    "        for j in range(n_res):\n",
    "            s_list.append(s_current)\n",
    "            s_current = s_current + dt_uniform * (A @ s_current + B @ u_interp[j])\n",
    "    \n",
    "    s_stack = torch.stack(s_list, dim=0)  # (n_res, n_s)\n",
    "    \n",
    "    # Compute loss: L = dt_uniform * sum_j (s_j^T Q s_j + u_j^T R u_j)\n",
    "    state_cost = torch.sum((s_stack @ Q) * s_stack)\n",
    "    input_cost = torch.sum((u_interp @ R) * u_interp)\n",
    "    \n",
    "    loss = dt_uniform * (state_cost + input_cost)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829ac61",
   "metadata": {},
   "source": [
    "### LHS - Substeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mnu9f0aymd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substep_loss(\n",
    "    inputs_qp,      # list of n tensors from QP solution, each shape (n_u,)\n",
    "    dts_torch,      # shape (n,), timesteps\n",
    "    s0,             # initial state, shape (n_s,)\n",
    "    A, B,           # system matrices (torch tensors)\n",
    "    Q, R,           # cost matrices (torch tensors)\n",
    "    n_sub=10,       # number of substeps per interval\n",
    "    use_exact=False # if True, use matrix_exp per interval\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute LQR cost with substeps within each non-uniform interval.\n",
    "    \n",
    "    This loss function:\n",
    "    1. For each interval k of duration dt_k, creates n_sub substeps\n",
    "    2. Applies constant input u_k throughout the interval\n",
    "    3. Integrates cost contribution from each substep\n",
    "    \n",
    "    Gradients flow through:\n",
    "    - inputs_qp -> QP solution -> dts_torch -> theta\n",
    "    - dt_subs = dts_torch / n_sub preserves gradients\n",
    "    \"\"\"\n",
    "    device = dts_torch.device\n",
    "    dtype = dts_torch.dtype\n",
    "    n = len(dts_torch)\n",
    "    \n",
    "    # Ensure tensors are on correct device\n",
    "    A = torch.as_tensor(A, dtype=dtype, device=device)\n",
    "    B = torch.as_tensor(B, dtype=dtype, device=device)\n",
    "    Q = torch.as_tensor(Q, dtype=dtype, device=device)\n",
    "    R = torch.as_tensor(R, dtype=dtype, device=device)\n",
    "    s0 = torch.as_tensor(s0, dtype=dtype, device=device)\n",
    "    \n",
    "    # Substep durations (gradients preserved through division)\n",
    "    dt_subs = dts_torch / n_sub  # (n,)\n",
    "    \n",
    "    # Expand inputs: each u_k repeated n_sub times\n",
    "    u_stack = torch.stack(inputs_qp, dim=0)  # (n, n_u)\n",
    "    u_expanded = u_stack.repeat_interleave(n_sub, dim=0)  # (n*n_sub, n_u)\n",
    "    \n",
    "    # Expand timesteps: each dt_sub_k repeated n_sub times\n",
    "    dt_expanded = dt_subs.repeat_interleave(n_sub)  # (n*n_sub,)\n",
    "    \n",
    "    total_substeps = n * n_sub\n",
    "    \n",
    "    # Forward simulation\n",
    "    s_list = []\n",
    "    s_current = s0.clone()\n",
    "    \n",
    "    if use_exact:\n",
    "        # Per-interval exact discretization\n",
    "        for k in range(n):\n",
    "            dt_sub_k = dt_subs[k]\n",
    "            Ad_k, Bd_k = zoh_discretize(dt_sub_k, A, B)\n",
    "            u_k = inputs_qp[k]\n",
    "            for _ in range(n_sub):\n",
    "                s_list.append(s_current)\n",
    "                s_current = Ad_k @ s_current + Bd_k @ u_k\n",
    "    else:\n",
    "        # Euler integration (vectorized loop)\n",
    "        for j in range(total_substeps):\n",
    "            s_list.append(s_current)\n",
    "            s_current = s_current + dt_expanded[j] * (A @ s_current + B @ u_expanded[j])\n",
    "    \n",
    "    s_stack = torch.stack(s_list, dim=0)  # (n*n_sub, n_s)\n",
    "    \n",
    "    # Compute loss: L = sum_j dt_sub_j * (s_j^T Q s_j + u_j^T R u_j)\n",
    "    state_cost = torch.sum((s_stack @ Q) * s_stack, dim=1)  # (n*n_sub,)\n",
    "    input_cost = torch.sum((u_expanded @ R) * u_expanded, dim=1)  # (n*n_sub,)\n",
    "    \n",
    "    loss = torch.sum(dt_expanded * (state_cost + input_cost))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alx0k0vexcr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gradient flow for hyper-sampling losses\n",
    "\n",
    "def test_gradient_flow():\n",
    "    \"\"\"Verify gradients flow correctly through both loss functions.\"\"\"\n",
    "    \n",
    "    n_test = 20\n",
    "    theta_test = torch.nn.Parameter(torch.ones(n_test, 1))\n",
    "    \n",
    "    A_torch = torch.tensor(A, dtype=torch.float32)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_torch = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_torch = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_torch = torch.tensor(s0, dtype=torch.float32)\n",
    "    \n",
    "    # Get timesteps via simplex transformation\n",
    "    dts_test = theta_2_dt(theta_test)\n",
    "    \n",
    "    # Create dummy inputs (simulate QP output with gradients)\n",
    "    inputs_test = [torch.randn(n_u, requires_grad=True) for _ in range(n_test)]\n",
    "    \n",
    "    # === Test Approach 1: Uniform Resampling ===\n",
    "    loss1 = uniform_resampling_loss(\n",
    "        inputs_test, dts_test, s0_torch,\n",
    "        A_torch, B_torch, Q_torch, R_torch,\n",
    "        T=T, n_res=100, use_exact=False\n",
    "    )\n",
    "    loss1.backward()\n",
    "    \n",
    "    print(\"Approach 1 (Uniform Resampling - Euler):\")\n",
    "    print(f\"  Loss value: {loss1.item():.6f}\")\n",
    "    print(f\"  theta.grad exists: {theta_test.grad is not None}\")\n",
    "    if theta_test.grad is not None:\n",
    "        print(f\"  theta.grad norm: {theta_test.grad.norm().item():.6f}\")\n",
    "    \n",
    "    # Reset gradients\n",
    "    theta_test.grad = None\n",
    "    for u in inputs_test:\n",
    "        if u.grad is not None:\n",
    "            u.grad = None\n",
    "    \n",
    "    # Recompute dts\n",
    "    dts_test = theta_2_dt(theta_test)\n",
    "    \n",
    "    # === Test Approach 2: Substeps ===\n",
    "    loss2 = substep_loss(\n",
    "        inputs_test, dts_test, s0_torch,\n",
    "        A_torch, B_torch, Q_torch, R_torch,\n",
    "        n_sub=10, use_exact=False\n",
    "    )\n",
    "    loss2.backward()\n",
    "    \n",
    "    print(\"\\nApproach 2 (Substeps - Euler):\")\n",
    "    print(f\"  Loss value: {loss2.item():.6f}\")\n",
    "    print(f\"  theta.grad exists: {theta_test.grad is not None}\")\n",
    "    if theta_test.grad is not None:\n",
    "        print(f\"  theta.grad norm: {theta_test.grad.norm().item():.6f}\")\n",
    "\n",
    "test_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yi74mu8fji",
   "metadata": {},
   "source": [
    "### Training with Hyper-Sampling Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oq9il0b3leo",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_hs = [\"uniform_resample\", \"substeps\"]\n",
    "\n",
    "if rerun:\n",
    "    n_epochs_hs = 400\n",
    "    n_res_hs = 1000  # for uniform resampling\n",
    "    n_sub_hs = 10    # for substeps\n",
    "    \n",
    "    history_hs = []\n",
    "    sol_hs = {}\n",
    "    \n",
    "    # Convert system matrices to torch tensors\n",
    "    A_torch = torch.tensor(A, dtype=torch.float32)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float32)\n",
    "    Q_torch = torch.tensor(Q, dtype=torch.float32)\n",
    "    R_torch = torch.tensor(R, dtype=torch.float32)\n",
    "    s0_torch = torch.tensor(s0, dtype=torch.float32)\n",
    "    \n",
    "    for method in loss_methods_hs:\n",
    "        print(f\"Training with method: {method}\")\n",
    "        \n",
    "        n = 160\n",
    "        \n",
    "        # Softmax-reparametrized timesteps\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "        \n",
    "        with tqdm(total=n_epochs_hs) as pbar:\n",
    "            for epoch in range(n_epochs_hs):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Get timesteps from simplex transformation\n",
    "                dts_torch = theta_2_dt(theta)\n",
    "                \n",
    "                # Solve QP via CVXPyLayer\n",
    "                pann_param_clqr, dts2 = create_pann_param_clqr_2(n)\n",
    "                cvxpylayer = CvxpyLayer(\n",
    "                    pann_param_clqr,\n",
    "                    parameters=[dts2],\n",
    "                    variables=s[1:n+1] + u[:n],\n",
    "                )\n",
    "                sol_hs[method] = cvxpylayer(dts_torch)\n",
    "                \n",
    "                # Extract inputs from QP solution\n",
    "                inputs_qp = [sol_hs[method][n+i] for i in range(n)]\n",
    "                \n",
    "                # Compute hyper-sampling loss\n",
    "                if method == \"uniform_resample\":\n",
    "                    loss = uniform_resampling_loss(\n",
    "                        inputs_qp, dts_torch, s0_torch,\n",
    "                        A_torch, B_torch, Q_torch, R_torch,\n",
    "                        T=T, n_res=n_res_hs, use_exact=False\n",
    "                    )\n",
    "                elif method == \"substeps\":\n",
    "                    loss = substep_loss(\n",
    "                        inputs_qp, dts_torch, s0_torch,\n",
    "                        A_torch, B_torch, Q_torch, R_torch,\n",
    "                        n_sub=n_sub_hs, use_exact=False\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {method}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "                history_hs.append({\n",
    "                    'method': method,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss.item(),\n",
    "                    'dts': dts_torch.detach().cpu().numpy(),\n",
    "                })\n",
    "        \n",
    "        print(f\"  Final loss: {history_hs[-1]['loss']:.6f}\\n\")\n",
    "    \n",
    "    with open(f\"{out_dir}/sol_hs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sol_hs, f)\n",
    "    with open(f\"{out_dir}/history_hs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history_hs, f)\n",
    "else:\n",
    "    with open(f\"{out_dir}/sol_hs.pkl\", \"rb\") as f:\n",
    "        sol_hs = pickle.load(f)\n",
    "    with open(f\"{out_dir}/history_hs.pkl\", \"rb\") as f:\n",
    "        history_hs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquv5t1gfie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for hyper-sampling methods\n",
    "n = 160\n",
    "\n",
    "for method in loss_methods_hs:\n",
    "    plot_training_res(\n",
    "        sol_hs[method], \n",
    "        [h for h in history_hs if h['method'] == method], \n",
    "        method=2\n",
    "    )\n",
    "    save_training_res(\n",
    "        method,\n",
    "        sol_hs[method],\n",
    "        [h for h in history_hs if h['method'] == method],\n",
    "        method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxozndbs5s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot: uniform resampling vs substeps\n",
    "fig, axs = plt.subplots(2, 2, figsize=(9.6, 6.4), constrained_layout=True)\n",
    "\n",
    "# Loss curves\n",
    "for method in loss_methods_hs:\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    axs[0, 0].plot([h['loss'] for h in history_method], label=method)\n",
    "axs[0, 0].set_xlabel(\"Epoch\")\n",
    "axs[0, 0].set_ylabel(\"Loss\")\n",
    "axs[0, 0].set_title(\"Loss Convergence\")\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Final timestep distributions\n",
    "for i, method in enumerate(loss_methods_hs):\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    d_arr = history_method[-1]['dts']\n",
    "    times = np.cumsum(d_arr)\n",
    "    axs[0, 1].plot(times, d_arr, label=method)\n",
    "axs[0, 1].set_xlabel(\"Time\")\n",
    "axs[0, 1].set_ylabel(\"Timestep duration\")\n",
    "axs[0, 1].set_title(\"Final Timestep Distributions\")\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Timestep histograms\n",
    "for i, method in enumerate(loss_methods_hs):\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    d_arr = history_method[-1]['dts']\n",
    "    axs[1, i].hist(d_arr.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "    axs[1, i].set_xlabel(\"Timestep duration\")\n",
    "    axs[1, i].set_ylabel(\"Count\")\n",
    "    axs[1, i].set_title(f\"Histogram: {method}\")\n",
    "    axs[1, i].axvline(T/n, color='r', linestyle='--', label=f\"uniform={T/n:.4f}\")\n",
    "    axs[1, i].legend()\n",
    "\n",
    "fig.suptitle(\"Comparison: Uniform Resampling vs Substeps\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6edd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate \"True\" Continuous-Time Cost\n",
    "\n",
    "def evaluate_continuous_cost(inputs_qp, dts, s0, A, B, Q, R, T, n_eval=10000):\n",
    "    \"\"\"\n",
    "    Evaluate the trajectory on a very dense grid to approximate true continuous cost.\n",
    "    This is the \"ground truth\" for comparison.\n",
    "    \"\"\"\n",
    "    A_t = torch.as_tensor(A, dtype=torch.float32)\n",
    "    B_t = torch.as_tensor(B, dtype=torch.float32)\n",
    "    Q_t = torch.as_tensor(Q, dtype=torch.float32)\n",
    "    R_t = torch.as_tensor(R, dtype=torch.float32)\n",
    "    s0_t = torch.as_tensor(s0, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(dts, torch.Tensor):\n",
    "        dts_np = dts.detach().cpu().numpy()\n",
    "    else:\n",
    "        dts_np = np.array(dts)\n",
    "\n",
    "    dt_eval = T / n_eval\n",
    "    t_cumsum = np.cumsum(dts_np)\n",
    "\n",
    "    # Simulate on very dense grid\n",
    "    s_current = s0_t.clone()\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for j in range(n_eval):\n",
    "        t_j = j * dt_eval\n",
    "        # Find which interval we're in\n",
    "        k = np.searchsorted(t_cumsum, t_j, side='right')\n",
    "        k = min(k, len(inputs_qp) - 1)\n",
    "\n",
    "        u_j = inputs_qp[k]\n",
    "        if isinstance(u_j, torch.Tensor):\n",
    "            u_j = u_j.detach()\n",
    "        else:\n",
    "            u_j = torch.tensor(u_j, dtype=torch.float32)\n",
    "\n",
    "        # Accumulate cost\n",
    "        state_cost = float(s_current @ Q_t @ s_current)\n",
    "        input_cost = float(u_j @ R_t @ u_j)\n",
    "        total_cost += dt_eval * (state_cost + input_cost)\n",
    "\n",
    "        # Euler step\n",
    "        s_current = s_current + dt_eval * (A_t @ s_current + B_t @ u_j)\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# Compare the two hyper-sampling methods on \"true\" continuous cost\n",
    "print(\"=== True Continuous-Time Cost Comparison ===\\n\")\n",
    "\n",
    "for method in loss_methods_hs:\n",
    "    sol = sol_hs[method]\n",
    "    history_method = [h for h in history_hs if h['method'] == method]\n",
    "    dts_final = history_method[-1]['dts']\n",
    "\n",
    "    inputs_qp = [sol[n+i] for i in range(n)]\n",
    "\n",
    "    true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "\n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Training loss (final): {history_method[-1]['loss']:.4f}\")\n",
    "    print(f\"  True continuous cost:  {true_cost:.4f}\")\n",
    "    print(f\"  dt range: [{np.min(dts_final):.5f}, {np.max(dts_final):.5f}]\")\n",
    "    print(f\"  dt std:   {np.std(dts_final):.5f}\")\n",
    "    print()\n",
    "\n",
    "# Also compare with existing methods if available\n",
    "print(\"=== Comparison with Other Methods ===\\n\")\n",
    "\n",
    "try:\n",
    "    for method in loss_methods_2:\n",
    "        sol = sol_rep[method]\n",
    "        history_method = [h for h in history_rep if h['method'] == method]\n",
    "        dts_final = history_method[-1]['dts']\n",
    "        inputs_qp = [sol[n+i] for i in range(n)]\n",
    "        true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "        print(f\"Reparametrized ({method}): true_cost = {true_cost:.4f}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for method in loss_methods_zoh:\n",
    "        sol = sol_zoh[method]\n",
    "        history_method = [h for h in history_zoh if h['method'] == method]\n",
    "        dts_final = history_method[-1]['dts']\n",
    "        inputs_qp = [sol[n+i] for i in range(n)]\n",
    "        true_cost = evaluate_continuous_cost(inputs_qp, dts_final, s0, A, B, Q, R, T)\n",
    "        print(f\"ZOH ({method}): true_cost = {true_cost:.4f}\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db1ad9",
   "metadata": {},
   "source": [
    "## DQP With ZOH Exact Discretization\n",
    "\n",
    "Use the exact discretization of the LTI system with zero-order hold (ZOH).\n",
    "\n",
    "OCP parameters:\n",
    "$$\n",
    "A_{d, k}, B_{d, k}\n",
    "= \\operatorname{ZOH}(A, B, \\Delta t_k)\n",
    "= \\exp \\left( \\begin{bmatrix} A & B \\\\ 0 & 0 \\end{bmatrix} \\Delta t_k \\right)\n",
    "= \\begin{bmatrix} A_{d, k} & B_{d, k} \\\\ 0 & I \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7816083",
   "metadata": {},
   "source": [
    "### No Cost Scaling With Interval Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad982d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "\n",
    "Aps = [cp.Parameter((n_s, n_s), name=f\"Ad_{k}\") for k in range(n)]\n",
    "Bps = [cp.Parameter((n_s, n_u), name=f\"Bd_{k}\") for k in range(n)]\n",
    "\n",
    "def create_exact_param_pann_clqr(n: int = N):\n",
    "    objective = cp.sum([cp.quad_form(s[k+1], Q) + cp.quad_form(u[k], R) for k in range(n)])\n",
    "    \n",
    "    constraints = [ s[k+1] == Aps[k] @ s[k] + Bps[k] @ u[k] for k in range(n) ] \\\n",
    "                + [ cp.abs(u[k]) <= u_max for k in range(n) ]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(objective), constraints)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "prob = create_exact_param_pann_clqr(n)\n",
    "assert prob.is_dpp()\n",
    "\n",
    "layer = CvxpyLayer(prob, parameters=Aps + Bps, variables=s[1:n+1] + u[:n])\n",
    "\n",
    "def Ad_Bd_from_dt(dt):\n",
    "    # Block matrix exponential per Van Loan\n",
    "    M = torch.zeros(n_s + n_u, n_s + n_u, dtype=torch.float32, device=dt.device)\n",
    "    M[:n_s, :n_s] = torch.tensor(A, dtype=torch.float32, device=dt.device) * dt\n",
    "    M[:n_s, n_s:] = torch.tensor(B, dtype=torch.float32, device=dt.device) * dt\n",
    "    E = torch.matrix_exp(M)\n",
    "    return E[:n_s, :n_s], E[:n_s, n_s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93654c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh = [\"unscaled\", \"time scaled\"]\n",
    "loss_methods_zoh = [\"time scaled\"]\n",
    "\n",
    "n_epochs_zoh = 1\n",
    "history_zoh = []\n",
    "sol_zoh = {}\n",
    "\n",
    "for method in loss_methods_zoh:\n",
    "    print(f\"ZOH Method: {method}\")\n",
    "\n",
    "    # Choose horizon for training (you can change to 200, etc.)\n",
    "    n = 160\n",
    "\n",
    "    # Softmax-reparam timesteps: theta -> dt on simplex (uses your theta_2_dt)\n",
    "    theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "    optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "    # Build the ZOH layer for this n\n",
    "    zoh_layer = layer\n",
    "\n",
    "    with tqdm(total=n_epochs_zoh) as pbar:\n",
    "        for epoch in range(n_epochs_zoh):\n",
    "            pbar.update(1)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Current timesteps from parameters (positive, sum to T)\n",
    "            dts_torch = theta_2_dt(theta)  # shape: (n,)\n",
    "\n",
    "            # Build (Ad,Bd) lists for the layer call\n",
    "            Ad_list, Bd_list = zip(*[Ad_Bd_from_dt(dt_k) for dt_k in dts_torch])\n",
    "\n",
    "            # Solve QP via CVXPYLayer forward pass\n",
    "            sol_zoh[method] = zoh_layer(*Ad_list, *Bd_list)\n",
    "\n",
    "            # Compute task loss (unscaled or time-scaled)\n",
    "            loss = task_loss_2(sol_zoh[method], dts_torch, method=method)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Log history\n",
    "            history_zoh.append({\n",
    "                \"method\": method,\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": float(loss.item()),\n",
    "                \"dts\": dts_torch.detach().cpu().numpy(),  # keep full grid\n",
    "            })\n",
    "\n",
    "n = 160\n",
    "for method in loss_methods_zoh:\n",
    "    plot_training_res(sol_zoh[method], [h for h in history_zoh if h['method'] == method], method=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh = [\"unscaled\", \"time scaled\"]\n",
    "loss_methods_zoh = [\"time scaled\"]\n",
    "\n",
    "if rerun:\n",
    "    n_epochs_zoh = 300\n",
    "    history_zoh = []\n",
    "    sol_zoh = {}\n",
    "\n",
    "    for method in loss_methods_zoh:\n",
    "        print(f\"ZOH Method: {method}\")\n",
    "\n",
    "        # Choose horizon for training (you can change to 200, etc.)\n",
    "        n = 160\n",
    "\n",
    "        # Softmax-reparam timesteps: theta -> dt on simplex (uses your theta_2_dt)\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        # Build the ZOH layer for this n\n",
    "        zoh_layer = layer\n",
    "\n",
    "        with tqdm(total=n_epochs_zoh) as pbar:\n",
    "            for epoch in range(n_epochs_zoh):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Current timesteps from parameters (positive, sum to T)\n",
    "                dts_torch = theta_2_dt(theta)  # shape: (n,)\n",
    "\n",
    "                # Build (Ad,Bd) lists for the layer call\n",
    "                Ad_list, Bd_list = zip(*[Ad_Bd_from_dt(dt_k) for dt_k in dts_torch])\n",
    "\n",
    "                # Solve QP via CVXPYLayer forward pass\n",
    "                sol_zoh[method] = zoh_layer(*Ad_list, *Bd_list)\n",
    "\n",
    "                # Compute task loss (unscaled or time-scaled)\n",
    "                loss = task_loss_2(sol_zoh[method], dts_torch, method=method)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Log history\n",
    "                history_zoh.append({\n",
    "                    \"method\": method,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": float(loss.item()),\n",
    "                    \"dts\": dts_torch.detach().cpu().numpy(),  # keep full grid\n",
    "                })\n",
    "                \n",
    "    with open(f\"{out_dir}/sol_zoh.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sol_zoh, f)\n",
    "    with open(f\"{out_dir}/history_zoh.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history_zoh, f)\n",
    "else:\n",
    "    with open(f\"{out_dir}/sol_zoh.pkl\", \"rb\") as f:\n",
    "        sol_zoh = pickle.load(f)\n",
    "    with open(f\"{out_dir}/history_zoh.pkl\", \"rb\") as f:\n",
    "        history_zoh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "for method in loss_methods_zoh:\n",
    "    plot_training_res(sol_zoh[method], [h for h in history_zoh if h['method'] == method], method=2)\n",
    "    save_training_res(\n",
    "        method.replace(\" \", \"_\") + \"_zoh\",\n",
    "        sol_zoh[method],\n",
    "        [h for h in history_zoh if h['method'] == method],\n",
    "        method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95475e32",
   "metadata": {},
   "source": [
    "#### WTF\n",
    "\n",
    "Check that the ZOH discretization is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = [sol_zoh[method][i] for i in range(n)]\n",
    "# inputs = [sol_zoh[method][n+i] for i in range(n)]\n",
    "# deltas = dts_torch.detach().cpu().numpy()\n",
    "\n",
    "# def shoot_dyn(s0, inputs, deltas):\n",
    "#     dt = 0.00001\n",
    "    \n",
    "#     time = 0.0\n",
    "    \n",
    "#     times = np.cumsum(deltas)\n",
    "    \n",
    "#     s = [s0]\n",
    "    \n",
    "#     while time <= T:\n",
    "#         i = np.searchsorted(times, time, side=\"right\") - 1\n",
    "\n",
    "#         s.append(s[-1] + (A @ s[-1] + B @ inputs[i].detach().cpu().numpy()) * dt)\n",
    "\n",
    "#         time = time + dt\n",
    "        \n",
    "#     return s\n",
    "    \n",
    "# s_hist = shoot_dyn(s0, inputs, deltas)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(np.array(s_hist))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38657d64",
   "metadata": {},
   "source": [
    "### With Cost Scaling With Interval Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12534e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "\n",
    "Aps = [cp.Parameter((n_s, n_s), name=f\"Ad_{k}\") for k in range(n)]\n",
    "Bps = [cp.Parameter((n_s, n_u), name=f\"Bd_{k}\") for k in range(n)]\n",
    "\n",
    "LQs = [cp.Parameter((n_s, n_s), PSD=True, name=f\"Q_{k}\") for k in range(n)]\n",
    "LRs = [cp.Parameter((n_u, n_u), PSD=True, name=f\"R_{k}\") for k in range(n)]\n",
    "\n",
    "def create_exact_param_pann_clqr_2(n: int = N):\n",
    "    objective = cp.sum([\n",
    "        cp.sum_squares(LQs[k] @ s[k+1]) + cp.sum_squares(LRs[k] @ u[k])\n",
    "        for k in range(n)\n",
    "    ])\n",
    "\n",
    "    constraints = [ s[k+1] == Aps[k] @ s[k] + Bps[k] @ u[k] for k in range(n) ] \\\n",
    "                + [ cp.abs(u[k]) <= u_max for k in range(n) ]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(objective), constraints)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "prob_2 = create_exact_param_pann_clqr_2(n)\n",
    "assert prob_2.is_dpp()\n",
    "\n",
    "layer = CvxpyLayer(prob_2, parameters=Aps + Bps + LQs + LRs, variables=s[1:n+1] + u[:n])\n",
    "\n",
    "def LQs_LRs_from_dt(dts):\n",
    "    Q_np = torch.as_tensor(Q, dtype=torch.float32)\n",
    "    R_np = torch.as_tensor(R, dtype=torch.float32)\n",
    "    LQ0  = torch.linalg.cholesky(Q_np)  # constant\n",
    "    LR0  = torch.linalg.cholesky(R_np)  # constant\n",
    "    LQs  = [torch.sqrt(dt) * LQ0 for dt in dts]\n",
    "    LRs  = [torch.sqrt(dt) * LR0 for dt in dts]\n",
    "    return LQs, LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_methods_zoh_2 = [\"time scaled\"]\n",
    "\n",
    "if rerun:\n",
    "    n_epochs_zoh = 500\n",
    "    history_zoh_2 = []\n",
    "    sol_zoh_2 = {}\n",
    "\n",
    "    for method in loss_methods_zoh_2:\n",
    "        print(f\"ZOH Method: {method}\")\n",
    "\n",
    "        # Choose horizon for training (you can change to 200, etc.)\n",
    "        n = 160\n",
    "\n",
    "        # Softmax-reparam timesteps: theta -> dt on simplex (uses your theta_2_dt)\n",
    "        theta = torch.nn.Parameter(torch.ones(n, 1))\n",
    "        optim = torch.optim.Adam([theta], lr=1e-2)\n",
    "\n",
    "        # Build the ZOH layer for this n\n",
    "        zoh_layer = layer\n",
    "\n",
    "        with tqdm(total=n_epochs_zoh) as pbar:\n",
    "            for epoch in range(n_epochs_zoh):\n",
    "                pbar.update(1)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Current timesteps from parameters (positive, sum to T)\n",
    "                dts_torch = theta_2_dt(theta)  # shape: (n,)\n",
    "\n",
    "                # Build (Ad,Bd) lists for the layer call\n",
    "                Ad_list, Bd_list = zip(*[Ad_Bd_from_dt(dt_k) for dt_k in dts_torch])\n",
    "                LQs_list, LRs_list = LQs_LRs_from_dt(dts_torch)\n",
    "\n",
    "                # Solve QP via CVXPYLayer forward pass\n",
    "                sol_zoh_2[method] = zoh_layer(*Ad_list, *Bd_list, *LQs_list, *LRs_list)\n",
    "\n",
    "                # Compute task loss (unscaled or time-scaled)\n",
    "                loss = task_loss_2(sol_zoh_2[method], dts_torch, method=method)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Log history\n",
    "                history_zoh_2.append({\n",
    "                    \"method\": method,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": float(loss.item()),\n",
    "                    \"dts\": dts_torch.detach().cpu().numpy(),  # keep full grid\n",
    "                })\n",
    "                \n",
    "    with open(f\"{out_dir}/sol_zoh_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sol_zoh_2, f)\n",
    "    with open(f\"{out_dir}/history_zoh_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history_zoh_2, f)\n",
    "else:\n",
    "    with open(f\"{out_dir}/sol_zoh_2.pkl\", \"rb\") as f:\n",
    "        sol_zoh_2 = pickle.load(f)\n",
    "    with open(f\"{out_dir}/history_zoh_2.pkl\", \"rb\") as f:\n",
    "        history_zoh_2 = pickle.load(f)\n",
    "n = 160\n",
    "for method in loss_methods_zoh_2:\n",
    "    plot_training_res(sol_zoh_2[method], [h for h in history_zoh_2 if h['method'] == method], method=2)\n",
    "    save_training_res(\n",
    "        method.replace(\" \", \"_\") + \"_zoh_2\",\n",
    "        sol_zoh_2[method],\n",
    "        [h for h in history_zoh_2 if h['method'] == method],\n",
    "        method=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wgpif9tahzc",
   "metadata": {},
   "source": [
    "## Sampling Density and Trajectory Change Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3gn63yn43bk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trajectory_data(ms, n):\n",
    "    \"\"\"Extract states, inputs, and timesteps from a method solution.\"\"\"\n",
    "    sol, history, sol_method = ms['sol'], ms['history'], ms.get('sol_method', 2)\n",
    "    \n",
    "    s_arr = np.array([sol[i].detach().numpy() for i in range(n)])\n",
    "    u_arr = np.array([sol[n+i].detach().numpy() for i in range(n)]).flatten()\n",
    "    \n",
    "    if sol_method == 1:\n",
    "        dts = np.concatenate([sol[2*n+i].detach().numpy() for i in range(n)]).flatten()\n",
    "    else:\n",
    "        dts = np.array(history[-1]['dts']).flatten()\n",
    "    \n",
    "    times = np.cumsum(dts)\n",
    "    return {'s': s_arr, 'u': u_arr, 'dts': dts, 'times': times}\n",
    "\n",
    "\n",
    "def compute_trajectory_metrics(data, n):\n",
    "    \"\"\"Compute sampling density and trajectory change metrics.\"\"\"\n",
    "    s, u, dts = data['s'], data['u'], data['dts']\n",
    "    dt_uniform = T / n\n",
    "    \n",
    "    return {\n",
    "        'sampling_density': (1.0 / dts) * dt_uniform,\n",
    "        'abs_u': np.abs(u),\n",
    "        'delta_u': np.abs(np.diff(u)),\n",
    "        'norm_s': np.linalg.norm(s, axis=1),\n",
    "        'delta_s': np.linalg.norm(np.diff(s, axis=0), axis=1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8zw8vf42u4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_and_changes(data, metrics, method_name, colors, axes=None):\n",
    "    \"\"\"Plot sampling density, |Delta u|, and ||Delta s|| on the same axes.\"\"\"\n",
    "    times = data['times']\n",
    "    ax = axes if axes is not None else plt.subplots(figsize=(3.2, 2.4))[1]\n",
    "    \n",
    "    ax.plot(times, metrics['sampling_density'], label=r'Sampling density', color=colors[0])\n",
    "    ax.plot(times[:-1], metrics['delta_u'], label=r'$|\\Delta u|$', color=colors[1])\n",
    "    ax.plot(times[:-1], metrics['delta_s'], label=r'$\\|\\Delta s\\|_2$', color=colors[2])\n",
    "    ax.axhline(1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set(ylabel='Value', title=method_name)\n",
    "    ax.legend(loc='upper right', fontsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46422jgtpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_correlation(x, y, max_lag=20):\n",
    "    \"\"\"Normalized cross-correlation. Positive lag: x[k] vs y[k+lag].\"\"\"\n",
    "    x = (x - np.mean(x)) / (np.std(x) + 1e-10)\n",
    "    y = (y - np.mean(y)) / (np.std(y) + 1e-10)\n",
    "    m = min(len(x), len(y))\n",
    "    lags = np.arange(-max_lag, max_lag + 1)\n",
    "    ccf = np.array([\n",
    "        np.mean(x[:m-lag] * y[lag:m]) if lag >= 0 else np.mean(x[-lag:m] * y[:m+lag])\n",
    "        for lag in lags if (m - abs(lag)) > 0\n",
    "    ])\n",
    "    return lags, ccf\n",
    "\n",
    "\n",
    "def plot_cross_correlations(data, metrics, method_name, colors, max_lag=30):\n",
    "    \"\"\"Plot cross-correlation between sampling density and trajectory metrics.\"\"\"\n",
    "    sd = metrics['sampling_density']\n",
    "    times = data['times']\n",
    "    n = len(sd)\n",
    "    sig_bound = 1.96 / np.sqrt(n)\n",
    "    \n",
    "    quantities = [\n",
    "        ('delta_u', r'$|\\Delta u|$', sd[:-1]),\n",
    "        ('delta_s', r'$\\|\\Delta s\\|_2$', sd[:-1]),\n",
    "        ('abs_u', r'$|u|$', sd),\n",
    "        ('norm_s', r'$\\|s\\|_2$', sd),\n",
    "        ('time', r'$t$', sd),\n",
    "    ]\n",
    "    \n",
    "    # Add time as a metric for correlation\n",
    "    metrics_with_time = {**metrics, 'time': times}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(quantities), figsize=(2.8 * len(quantities), 2.8))\n",
    "    \n",
    "    for ax, (key, label, sd_aligned) in zip(axs, quantities):\n",
    "        metric_vals = metrics_with_time[key]\n",
    "        if len(metric_vals) > len(sd_aligned):\n",
    "            metric_vals = metric_vals[:len(sd_aligned)]\n",
    "        lags, ccf = compute_cross_correlation(sd_aligned, metric_vals, max_lag=max_lag)\n",
    "        ax.plot(lags, ccf, color=colors[0], marker='o', markersize=2)\n",
    "        ax.axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
    "        ax.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "        ax.axhline(sig_bound, color=colors[5], linestyle=':', alpha=0.5)\n",
    "        ax.axhline(-sig_bound, color=colors[5], linestyle=':', alpha=0.5)\n",
    "        ax.set(xlabel='Lag', ylabel='CCF', title=f'SD vs {label}')\n",
    "        \n",
    "        peak_idx = np.argmax(np.abs(ccf))\n",
    "        peak_lag, peak_val = lags[peak_idx], ccf[peak_idx]\n",
    "        \n",
    "        y_offset = -25 if peak_val > 0 else 15\n",
    "        ax.annotate(f'{peak_val:.2f} @ {peak_lag}',\n",
    "                    xy=(peak_lag, peak_val), fontsize=8,\n",
    "                    xytext=(0, y_offset), textcoords='offset points',\n",
    "                    ha='center', bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.7))\n",
    "    \n",
    "    fig.suptitle(f'{method_name}', fontsize=11)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlwkc2i3n3",
   "metadata": {},
   "source": [
    "### Collect All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yg6p9y0y0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "method_solutions = {}\n",
    "\n",
    "method_configs = [\n",
    "    (\"Aux\", loss_methods, sol_aux, history_aux, 1),\n",
    "    (\"Rep\", loss_methods_2, sol_rep, history_rep, 2),\n",
    "    (\"HS\", loss_methods_hs, sol_hs, history_hs, 2),\n",
    "    (\"ZOH\", loss_methods_zoh, sol_zoh, history_zoh, 2),\n",
    "    (\"ZOH2\", loss_methods_zoh_2, sol_zoh_2, history_zoh_2, 2),\n",
    "]\n",
    "\n",
    "for prefix, methods, sol_dict, history_list, sol_method in method_configs:\n",
    "    try:\n",
    "        for method in methods:\n",
    "            key = f\"{prefix}: {method}\"\n",
    "            method_solutions[key] = {\n",
    "                'sol': sol_dict[method],\n",
    "                'history': [h for h in history_list if h['method'] == method],\n",
    "                'sol_method': sol_method,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {prefix} methods: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(method_solutions)} methods: {list(method_solutions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8dkp4o5rq",
   "metadata": {},
   "source": [
    "### Sampling Density vs Trajectory Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcpdmaqly6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_methods = len(method_solutions)\n",
    "n_rows = int(np.ceil(n_methods / 2))\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(10, 2.5 * n_rows), squeeze=False)\n",
    "\n",
    "for i, (key, ms) in enumerate(method_solutions.items()):\n",
    "    data = extract_trajectory_data(ms, n)\n",
    "    metrics = compute_trajectory_metrics(data, n)\n",
    "    plot_density_and_changes(data, metrics, key, colors, axes=axs[i // 2, i % 2])\n",
    "\n",
    "for j in range(i + 1, n_rows * 2):\n",
    "    fig.delaxes(axs[j // 2, j % 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmte4d32x",
   "metadata": {},
   "source": [
    "### Cross-Correlation (Time-Lagged)\n",
    "\n",
    "Cross correlation of the Sampling Density (SD) with:\n",
    "- $\\| \\Delta u \\|$\n",
    "- $\\| \\Delta s \\|_2$\n",
    "- $\\| u \\|$\n",
    "- $\\| s \\|_2$\n",
    "\n",
    "On the y-axis, the cross-correlation factor.\n",
    "\n",
    "The box indicates the maximum CC and the time lag at which it happens.\n",
    "\n",
    "The dotted lines at $\\pm 1.96 / \\sqrt{n}$ â‰ˆ Â±0.155 (for n=160) are the 95% CI. Outside bounds -> statistically significant correlation.\n",
    "\n",
    "- **lag = 0**: Instantaneous correlation\n",
    "- **lag > 0**: Does high sampling density *precede* large changes?\n",
    "- **lag < 0**: Does high sampling density *follow* large changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d7gl5gsxr",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ms in method_solutions.items():\n",
    "    data = extract_trajectory_data(ms, n)\n",
    "    metrics = compute_trajectory_metrics(data, n)\n",
    "    plot_cross_correlations(data, metrics, key, colors, max_lag=30)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
